{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasom222g/learn-LLM/blob/main/06_7_%E1%84%92%E1%85%A1%E1%86%AB%E1%84%80%E1%85%AE%E1%86%A8%E1%84%8B%E1%85%A5_%E1%84%87%E1%85%A5%E1%86%B8%E1%84%85%E1%85%B2%E1%86%AF_Q%26A_%E1%84%8E%E1%85%A2%E1%86%BA%E1%84%87%E1%85%A9%E1%86%BA(QLorRA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQarPV-Mfeam"
      },
      "source": [
        "# 한국어 챗봇 구현 및 최적화 LLM 실습\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1️⃣ 실습 개요\n",
        "\n",
        "이 노트북은 Google Colab 환경에서 Unsloth 기반 LLM을 활용하여 **한국어 정보제공 챗봇을 구현하고 최적화하는 실습**을 제공합니다.  \n",
        "4-bit QLoRA 기반 경량 파인튜닝과 RAG(Retrieval-Augmented Generation)를 적용하여, **적은 GPU 자원에서도 실용적인 한국어 응답형 챗봇**을 구축하는 과정을 다룹니다.\n",
        "\n",
        "---\n",
        "\n",
        "2️⃣ 사용할 핵심 기술\n",
        "\n",
        "- **Unsloth 최적화 모델 활용**  \n",
        "  - Meta의 Llama 3.1, Qwen2.5, Mistral 등 최신 모델을 2배 빠른 속도, 적은 VRAM으로 파인튜닝할 수 있도록 최적화한 라이브러리입니다.  \n",
        "  - Transformers 대비 빠른 추론 및 효율적인 실습 가능.\n",
        "\n",
        "- **QLoRA 기반 경량 파인튜닝**  \n",
        "  - 4-bit 양자화 모델을 기반으로 하여 **VRAM 15GB 이하 환경에서도 고성능 모델 파인튜닝**이 가능하며, Unsloth에서는 QLoRA 정확도 손실도 거의 없습니다.\n",
        "\n",
        "- **한국어 QA 데이터셋 활용**  \n",
        "  - `jihye-moon/LawQA-Ko` 데이터셋을 사용하여, 법률 도메인에 특화된 챗봇을 제작해보겠습니다.\n",
        "\n",
        "- **RAG 적용 (LangChain + ChromaDB)**  \n",
        "  - Fine-tuning으로도 커버하기 어려운 최신 지식 또는 외부 문서 기반 응답을 위한 **검색 결합형 챗봇 구조(RAG)**도 실습합니다.\n",
        "\n",
        "---\n",
        "\n",
        "3️⃣ 최종 목표\n",
        "\n",
        "- ✅ 한국어 기반의 정보제공형 LLM 챗봇 구현(법률)\n",
        "- ✅ QLoRA 방식을 활용한 경량 파인튜닝 실습 진행\n",
        "- ✅ LangChain 기반의 RAG 검색 기능을 통해 응답의 품질과 정보성 향상\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_l8KOawfto2"
      },
      "source": [
        "## 1) Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPTOTdZfyX3"
      },
      "source": [
        "1. Unsloth란?\n",
        "\n",
        "Unsloth는 오픈소스 경량화 모델 최적화 라이브러리로,  \n",
        "Meta의 Llama 3, Google의 Gemma, Microsoft의 Phi-4, Mistral 등의 최신 모델을  \n",
        "**더 빠르고 적은 VRAM으로 파인튜닝**할 수 있도록 설계된 툴킷입니다.  \n",
        "\n",
        "특히 Hugging Face의 `transformers`, `trl` 기반과 호환되며,  \n",
        "**4-bit 및 8-bit QLoRA**를 활용해 최소한의 메모리로도 고성능 학습이 가능한 것이 장점입니다.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "2. Unsloth의 주요 특징\n",
        "\n",
        "- 기존 방식 대비 **2배 빠른 파인튜닝 속도**\n",
        "- **VRAM 사용량 70~80% 절감** 가능 (4-bit QLoRA)\n",
        "- 4-bit 및 8-bit **Quantization 지원**\n",
        "- **Llama, Qwen, Gemma, Mistral 등 다양한 모델 지원**\n",
        "- **GGUF, Ollama, vLLM** 등 다양한 포맷 변환 가능\n",
        "- Hugging Face 및 LangChain과 호환\n",
        "- Colab에서도 무료로 실행 가능 (로컬 및 저사양 환경도 가능)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "- 이 실습에서는 한국어 응답에 최적화된 **`MLP-KTLim/llama-3-Korean-Bllossom-8B`** 모델을 사용합니다.  \n",
        "- 이는 Llama 3.1 기반으로 한국어 Instruction 기반 튜닝이 되어 있어,  \n",
        "- 별도의 다국어 모델 설정 없이도 자연스러운 한국어 질문-응답이 가능합니다.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "참고 링크\n",
        "\n",
        "- [Unsloth Guide (공식 문서)](https://docs.unsloth.ai/get-started/fine-tuning-guide)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g930SdEAk0g0"
      },
      "source": [
        "아래 코드 실행 후 '런타임 - 세션 다시 시작'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AhQpQKo-On33",
        "outputId": "9964bdbe-c9cd-470d-f8b8-451cedec7cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.3.18-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.3.14 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.3.16-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.50.0)\n",
            "Collecting datasets>=2.16.0 (from unsloth)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.5.2)\n",
            "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Collecting protobuf<4.0.0 (from unsloth)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.29.3)\n",
            "Collecting hf_transfer (from unsloth)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.16.0->unsloth)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.3.14->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.3.18-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.3.16-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.9/126.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.17-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, shtab, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf_transfer, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, datasets, xformers, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.4 cut_cross_entropy-25.1.1 datasets-3.4.1 dill-0.3.8 fsspec-2024.12.0 hf_transfer-0.1.9 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 protobuf-3.20.3 shtab-1.7.1 trl-0.15.2 tyro-0.9.17 unsloth-2025.3.18 unsloth_zoo-2025.3.16 xformers-0.0.29.post3 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "68b5ff43dd904bfda51decf87cf73bb2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKnqNRLMgM_L"
      },
      "source": [
        "## 2) 기본 챗봇 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33z9dUXRgJ-c"
      },
      "source": [
        "### (1) 모델 로드 및 기본 설정\n",
        "\n",
        "본 실습에서는 Hugging Face에서 제공하는 `MLP-KTLim/llama-3-Korean-Bllossom-8B` 모델을 활용하여 한국어 챗봇을 구축합니다.\n",
        "\n",
        "\n",
        "[MLP-KTLim/llama-3-Korean-Bllossom-8B](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B)\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "이 모델의 가장 큰 특징은 한국어에 특화되어 있다는 점입니다.\n",
        "\n",
        "**한국어 특화 모델**\n",
        "   - LLaMA-3 기반으로 한국어에 특화된 모델이며, 다양한 한국어 질의에 자연스럽게 응답이 가능합니다.\n",
        "   -  Llama3대비 대략 25% 더 긴 길이의 한국어 Context 처리가능\n",
        "   - 한국어-영어 Pararell Corpus를 활용한 한국어-영어 지식연결 (사전학습)\n",
        "   - 한국어 문화, 언어를 고려해 언어학자가 제작한 데이터를 활용한 미세조정\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e0db5dbe4a5642b286ac8e6dfe1f9ac0",
            "68e6c02e1ec043e3b1dee1ba46ebf812",
            "f1e90821f3af4129bc6b2ec21e109faa",
            "c4182993cae747b5b8faa3cabe406016",
            "155ba77be1a449c6afb12ebfc0309442",
            "600d691a3f4e494bb59e4fb02690c14d",
            "21b6d8f66fe5426f9a5e21f2edf3900f",
            "7d2a925ac30e4c9d888c81b20f7a805c",
            "e9c553dea86d4c319e195d90e7c50562",
            "8febbb44d6ea44f8804019a00881fe2e",
            "d4985b03da554fd3ac58e0aa642d0cc2"
          ],
          "height": 568
        },
        "id": "euNQra_o3RK4",
        "outputId": "2ae4634c-ed29-497a-e446-a12a174e00b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0db5dbe4a5642b286ac8e6dfe1f9ac0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5541e5015991>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# ✅ 사용 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# ✅ 권장 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    574\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4453\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4454\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4455\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, low_cpu_mem_usage, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, device_mesh, key_mapping, weights_only, _fast_init)\u001b[0m\n\u001b[1;32m   4882\u001b[0m                 \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4883\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4884\u001b[0;31m                     disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4885\u001b[0m                         \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4886\u001b[0m                         \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mto_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\",  # ✅ 사용 모델\n",
        "    max_seq_length = 2048,      # ✅ 권장 설정\n",
        "    dtype = None,               # ✅ float16 or bfloat16 자동 선택\n",
        "    load_in_4bit = True         # ✅ 4bit QLoRA 양자화 로딩 비활성화\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfrKW_06_npk"
      },
      "source": [
        "### (2) 기본 챗봇 응답 생성 과정\n",
        "\n",
        "\n",
        "- 파인튜닝 전에 사전 학습된 모델의 기본 성능을 확인합니다.\n",
        "- 사용자 질문에 대한 응답을 generate()로 직접 생성해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3_Ehn-v_K4k"
      },
      "source": [
        "- model.generate : 모델이 응답을 생성할 때 품질을 조절하는 다양한 하이퍼파라미터를 설정합니다.\n",
        "    - 주요 파라미터\n",
        "        - temperature : 예측값에 무작위성 부여, 낮을수록 일관적인 응답, 높을수록 창의적 응답.\n",
        "          0.5가 일반적이며 답변을\n",
        "        - top_p : 누적 확률 기준으로 상위 토큰만 샘플링. 낮을수록 안정적, 높을수록 다양함. (각 토큰들의 예측값의 합에 해당하는 토큰만 랜덤 샘플링 ex. 강아지 0.7 + 개 0.2 )\n",
        "        - repetition_penalty : 같은 문장이나 단어 반복을 억제. 자연스러운 응답 유도.\n",
        "        - do_sample : 샘플링 방식을 적용하여 다양한 답변 생성 가능 (True일 경우 랜덤성 부여).\n",
        "        - max_new_tokens : 생성되는 응답의 최대 토큰 수를 제한 (응답 길이 제어).\n",
        "\n",
        "- 무작위성이 높다 -> 예측값이 평탄화 되었다.\n",
        "- 무작위성이 높을수록\n",
        "  - 일관성 ⬇️\n",
        "  - 다양성 ⬆️ 창의성 ⬆️\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThZbsQBO_cwo",
        "outputId": "6994e27c-239f-4247-9b34-31f0d849bd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 질문: 최저임금제란 무엇인가요?\n",
            "🤖 응답: 최저임금제란 무엇인가요? 최소한의 임금을 의미하는 것입니다. 이는 기업이 직원에게 필요한 기본적인 생활 수준을 보장하기 위해 지급해야 할 최저한의 급여를 말합니다. 예를 들어, 특정 지역에서 공공적으로 인정되는 최저임금은 8시간 근로에 대해 월평으로 최소 $15 이상을 지급하도록 요구할 수 있습니다. 이러한 기준은 노동자의 생계비용과 사회적 보호를 고려하여\n"
          ]
        }
      ],
      "source": [
        "user_input = \"최저임금제란 무엇인가요?\"\n",
        "\n",
        "# ✅ 입력을 모델이 처리할 수 있는 텐서로 변환\n",
        "inputs = tokenizer(user_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(f\"📝 질문: {user_input}\")\n",
        "\n",
        "# ✅ 모델 응답 생성 (추론 옵션 적용)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,        # 응답 최대 길이 제한\n",
        "    temperature=0.5,           # 무작위성의 온도 조절을 통해 일관된 응답 유도(낮을수록 일관성 높음, 기본값: 1, 1보다 높게주면 무작위성 증가)\n",
        "    top_p=0.85,                # 상위 확률 토큰만 샘플링\n",
        "    repetition_penalty=1.2,    # 반복 방지\n",
        "    do_sample=True             # 샘플링 방식 적용\n",
        ")\n",
        "\n",
        "# ✅ 응답 디코딩\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"🤖 응답:\", decoded_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_WHmc6VAhaD",
        "outputId": "5a47884e-d3ff-4fab-d154-d77063172547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 질문: 최저임금제도란 무엇이며, 최저임금액은 어떻게 결정되나요?\n",
            "\n",
            "](https://blog.naver.com/jeongwooch/230): 최저임금제도의 필요성과 문제점을 다루고 있습니다.<|end_of_text|>\n",
            "\n",
            "✅ 스트리밍 응답 완료!\n",
            "🤖 응답: 최저임금제도란 무엇이며, 최저임금액은 어떻게 결정되나요?](https://blog.naver.com/jeongwooch/230): 최저임금제도의 필요성과 문제점을 다루고 있습니다.\n"
          ]
        }
      ],
      "source": [
        "user_input = \"최저임금제도란 무엇이며, 최저임금액은 어떻게 결정되나요?\"\n",
        "\n",
        "# ✅ 입력 텐서 변환\n",
        "inputs = tokenizer(user_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(f\"📝 질문: {user_input}\\n\")\n",
        "\n",
        "# ✅ 실시간 출력 스트리머\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# ✅ 모델 응답 생성\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.5,\n",
        "    top_p=0.85,\n",
        "    repetition_penalty=1.2,\n",
        "    do_sample=True,\n",
        "    streamer=text_streamer\n",
        ")\n",
        "\n",
        "print(\"\\n✅ 스트리밍 응답 완료!\")\n",
        "# ✅ 응답 디코딩\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"🤖 응답:\", decoded_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_oanPDoM9VQ"
      },
      "source": [
        "기본적으로 괜찮은 답변을 보여줍니다.\n",
        "\n",
        "이 모델을 법률데이터로 finetuning해서 더 법률에 특화되도록 만들겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGH6GKrCf32d"
      },
      "source": [
        "## 3) LoRA(QLoRA)를 활용한 모델 경량 미세튜닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sB6ynVegXL1"
      },
      "source": [
        "### (1) 한국어 데이터셋 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsgjq9N_c5Ba"
      },
      "source": [
        "이번 실습에서는 한국어 법률 정보 챗봇을 구축하기 위해, Hugging Face에서 제공하는 jihye-moon/LawQA-Ko 데이터셋을 사용합니다.\n",
        "\n",
        "이 데이터셋은 실제 법률 문서를 기반으로 구성된 질문-답변 쌍으로, 법적 용어, 판례, 계약 관련 문의 등 도메인 특화된 내용을 담고 있습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "이 데이터셋은 AI Hub의 질문-답변 데이터를 정제한 것으로,\n",
        "질문과 답변이 명확히 구분되어 있고, 정보 전달에 적합한 구조를 갖추고 있습니다.\n",
        "- load_dataset() 함수로 바로 불러올 수 있어 실습에 편리합니다.\n",
        "- 한국어 질문-답변 형식으로 구성되어 있으며, Alpaca-style 포맷으로 가공해 fine-tuning에 적합합니다.\n",
        "- 약 14,800건 이상의 학습 데이터가 포함되어 있어 경량 튜닝에도 충분한 양을 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "6e0ea9088abc4e5eb378503e294afe01",
            "0fa6847ca1714a7b81be18977d827ab3",
            "f0cb596680b445f69bdc59d0978fab1a",
            "67a5942c0247481fa8c907f6e703a1de",
            "6e47ea66d19b4b72b1ef3b3d067af00d",
            "3747d95221334327bad3300eaa530e4f",
            "b80db5254f974815b051ff95487e6001",
            "171761887dd8496ead885e6f8db5ad77",
            "5a8b794f078e4fbca6f24c2c8be73d4f",
            "836cc33d9a584660a4ebb64dfbedc44f",
            "4eda05ba1e754cda92b5b9215ec0dbb8",
            "7de99cddaf0a4de79d45b7629522424e",
            "8d466f81bea74df79c674433039ee962",
            "b8ae688e7d0c478a86ffa0d955504da1",
            "37b8dd12f46040edada15e5da27e9521",
            "cd1d93c5e03e4562b4c048fc13242a63",
            "6ab86de505a845119c67b6b85e9fb20f",
            "ef0f7b58f37a47a5a2b56b6f8e250fbc",
            "0fd8419c411a435da71122f464464ab6",
            "0d48a733aad9430ab730ef1ddfefcafd",
            "892d2f9b6ea44980a5dd2fc5c283d0c9",
            "1bdf636d0f8041df8b918ea5dd667ba3",
            "20d59878c1da46a78f9505d59562f2f4",
            "5bcb5e7e2f1444a9bd033181830132fb",
            "f02842207877440594c1c65b5168e1a8",
            "9505f13fa4884e4db0dbbd39dfaf77bf",
            "1a17282e28b046d69087b513fe036dc0",
            "5eea03459e784d91b773e965871a6f11",
            "0889d33fc7f847b594a65d54238bc8af",
            "89feafd427054373863d924c4afb0b2f",
            "4b56edd3fc1441e98ba898d6fec53fe1",
            "8cb09551781d4fc3905824aa1a5cea7d",
            "825a20c07bdd429b9e45dcee72e7e07f"
          ]
        },
        "id": "g2j8_mMkaBjZ",
        "outputId": "12b68244-d8f0-40fc-83e9-6ef5cdcc81a4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e0ea9088abc4e5eb378503e294afe01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "law_qa_dataset.jsonl:   0%|          | 0.00/32.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7de99cddaf0a4de79d45b7629522424e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/14819 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20d59878c1da46a78f9505d59562f2f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': '최저임금제도란 무엇이며, 최저임금액은 어떻게 결정되는지요?', 'precedent': '', 'answer': '최저임금제도란 국가가 임금액의 최저한도를 정하여 사용자에게 이를 준수하도록 강제하는 제도를 말합니다. 그러므로 최저임금액이 결정·고시되면 사용자는 근로자와 합의하여 최저임금액보다 낮은 임금을 지급하기로 약정하더라도 그것은 당연히 무효가 되며, 고용노동부장관이 고시한 최저임금액 이상을 지급하여야 합니다(최저임금법 제6조). 최저임금제도의 적용대상은 근로자를 사용하는 모든 사업 또는 사업장에 적용되며, 상용근로자 뿐만 아니라 임시근로자나 일용근로자, 시간제근로자 등 모든 근로자에게 적용됩니다. 다만, 동거의 친족만을 사용하는 사업과 가사사용인에 대하여는 적용하지 아니하고, 선원법의 적용을 받는 선원 및 선원을 사용하는 선박의 소유자에 대하여는 이를 적용하지 아니합니다(같은 법 제3조). 그리고 수습사용 중에 있는 자로서 수습사용한 날부터 3월 이내인 근로자는 시간급 최저임금액의 90%를 지급할 수 있고, 사용자가 고용노동부장관의 인가를 받은 감시 또는 단속적으로 근로에 종사하는 자(수위, 경비원, 자가용운전기사 등)는 시간급 최저임금액의 80%를 지급할 수 있습니다(같은 법 제5조 제2항, 같은 법 시행령 제3조).그러나 같은 법 시행령 제6조는 사용자는 고용노동부장관의 인가를 받아 ‘근로자의 정신 또는 신체의 장애가 당해 근로자를 종사시키고자 하는 업무의 수행에 직접적으로 현저한 지장을 주는 것이 명백하다고 인정되는 자’에 대하여는 최저임금의 적용을 제외할 수 있도록 규정하고 있습니다.고용노동부장관은 매년 3월 31일까지 근로자위원, 사용자위원, 공익위원 등으로 구성된 최저임금심의위원회에 최저임금에 관한 심의를 요청하여야 하고, 동 위원회에서는 근로자의 생계비, 유사근로자의 임금, 노동생산성 및 소득분배율 등을 고려하여 최저임금안을 심의하며, 심의위원회로부터 최저임금안을 제출받은 때에는 지체없이 사업의 종류별 최저임금안 및 적용사업의 범위를 고시하여야 하며, 매년 8월 5일까지 최저임금을 결정하여야 합니다(같은 법 제4조, 제5조, 제8조, 제9조, 같은 법 시행령 제7조).참고로 고용노동부에서 고시한 2016년 1월 1일부터 2016년 12월 31일 까지 적용되는 최저임금액을 보면 시간급은 6,030원으로 정하고 있습니다.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# jihye-moon/LawQA-Ko 데이터셋을 train split 기준으로 불러옵니다.\n",
        "dataset = load_dataset(\"jihye-moon/LawQA-Ko\", split=\"train\")\n",
        "\n",
        "\n",
        "# ✅ 첫 번째 샘플 확인\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bExAqvqeIYxk"
      },
      "source": [
        "jihye-moon/LawQA-Ko 데이터셋은 다음과 같은 필드를 갖고 있습니다:\n",
        "- question: 사용자 질문\n",
        "- answer: 답변 (자세한 법률 설명 포함)\n",
        "\n",
        "<br>\n",
        "하지만 파인튜닝을 위해서는 아래와 같은 Alpaca 포맷으로 변경해야 합니다:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"instruction\": \"질문 내용\",\n",
        "    \"input\": \"\",  // 선택사항 (없으면 빈 문자열)\n",
        "    \"output\": \"답변 내용\",\n",
        "    \"text\": \"Alpaca-style 데이터 포맷으로 구성된 최종 학습 문장\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbPetbP9Iz4h"
      },
      "source": [
        "🧾 Alpaca-style 데이터 포맷이란?\n",
        "\n",
        "Alpaca-style 포맷은 LLM을 Instruction 기반으로 파인튜닝할 때 사용하는 대표적인 형식입니다.\n",
        "모델이 어떤 명령을 수행해야 하는지 명확히 알려주는 구조로, 특히 정보 제공형 단일턴 챗봇에 적합합니다.\n",
        "\n",
        "\n",
        "- 형식\n",
        "```\n",
        "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {instruction}\n",
        "\n",
        "    ### Response:\n",
        "    {output}<|end_of_text|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IpkfKbIvYdl-"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ✅ 데이터셋 로드\n",
        "dataset = load_dataset(\"jihye-moon/LawQA-Ko\", split=\"train\")\n",
        "\n",
        "# ✅ EOS 토큰\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# ✅ Alpaca-style 프롬프트 템플릿\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7TQXnsTE_JkP"
      },
      "outputs": [],
      "source": [
        "# ✅ Step 1: instruction / output 필드 생성\n",
        "def to_alpaca_format(example):\n",
        "    return {\n",
        "        \"instruction\": example[\"question\"],\n",
        "        \"output\": example[\"answer\"]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(to_alpaca_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjFdenYI_Ogy",
        "outputId": "1d62b4fd-031c-4d12-f45d-b4cdfdcc1ac5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '최저임금제도란 무엇이며, 최저임금액은 어떻게 결정되는지요?',\n",
              " 'precedent': '',\n",
              " 'answer': '최저임금제도란 국가가 임금액의 최저한도를 정하여 사용자에게 이를 준수하도록 강제하는 제도를 말합니다. 그러므로 최저임금액이 결정·고시되면 사용자는 근로자와 합의하여 최저임금액보다 낮은 임금을 지급하기로 약정하더라도 그것은 당연히 무효가 되며, 고용노동부장관이 고시한 최저임금액 이상을 지급하여야 합니다(최저임금법 제6조). 최저임금제도의 적용대상은 근로자를 사용하는 모든 사업 또는 사업장에 적용되며, 상용근로자 뿐만 아니라 임시근로자나 일용근로자, 시간제근로자 등 모든 근로자에게 적용됩니다. 다만, 동거의 친족만을 사용하는 사업과 가사사용인에 대하여는 적용하지 아니하고, 선원법의 적용을 받는 선원 및 선원을 사용하는 선박의 소유자에 대하여는 이를 적용하지 아니합니다(같은 법 제3조). 그리고 수습사용 중에 있는 자로서 수습사용한 날부터 3월 이내인 근로자는 시간급 최저임금액의 90%를 지급할 수 있고, 사용자가 고용노동부장관의 인가를 받은 감시 또는 단속적으로 근로에 종사하는 자(수위, 경비원, 자가용운전기사 등)는 시간급 최저임금액의 80%를 지급할 수 있습니다(같은 법 제5조 제2항, 같은 법 시행령 제3조).그러나 같은 법 시행령 제6조는 사용자는 고용노동부장관의 인가를 받아 ‘근로자의 정신 또는 신체의 장애가 당해 근로자를 종사시키고자 하는 업무의 수행에 직접적으로 현저한 지장을 주는 것이 명백하다고 인정되는 자’에 대하여는 최저임금의 적용을 제외할 수 있도록 규정하고 있습니다.고용노동부장관은 매년 3월 31일까지 근로자위원, 사용자위원, 공익위원 등으로 구성된 최저임금심의위원회에 최저임금에 관한 심의를 요청하여야 하고, 동 위원회에서는 근로자의 생계비, 유사근로자의 임금, 노동생산성 및 소득분배율 등을 고려하여 최저임금안을 심의하며, 심의위원회로부터 최저임금안을 제출받은 때에는 지체없이 사업의 종류별 최저임금안 및 적용사업의 범위를 고시하여야 하며, 매년 8월 5일까지 최저임금을 결정하여야 합니다(같은 법 제4조, 제5조, 제8조, 제9조, 같은 법 시행령 제7조).참고로 고용노동부에서 고시한 2016년 1월 1일부터 2016년 12월 31일 까지 적용되는 최저임금액을 보면 시간급은 6,030원으로 정하고 있습니다.',\n",
              " 'instruction': '최저임금제도란 무엇이며, 최저임금액은 어떻게 결정되는지요?',\n",
              " 'output': '최저임금제도란 국가가 임금액의 최저한도를 정하여 사용자에게 이를 준수하도록 강제하는 제도를 말합니다. 그러므로 최저임금액이 결정·고시되면 사용자는 근로자와 합의하여 최저임금액보다 낮은 임금을 지급하기로 약정하더라도 그것은 당연히 무효가 되며, 고용노동부장관이 고시한 최저임금액 이상을 지급하여야 합니다(최저임금법 제6조). 최저임금제도의 적용대상은 근로자를 사용하는 모든 사업 또는 사업장에 적용되며, 상용근로자 뿐만 아니라 임시근로자나 일용근로자, 시간제근로자 등 모든 근로자에게 적용됩니다. 다만, 동거의 친족만을 사용하는 사업과 가사사용인에 대하여는 적용하지 아니하고, 선원법의 적용을 받는 선원 및 선원을 사용하는 선박의 소유자에 대하여는 이를 적용하지 아니합니다(같은 법 제3조). 그리고 수습사용 중에 있는 자로서 수습사용한 날부터 3월 이내인 근로자는 시간급 최저임금액의 90%를 지급할 수 있고, 사용자가 고용노동부장관의 인가를 받은 감시 또는 단속적으로 근로에 종사하는 자(수위, 경비원, 자가용운전기사 등)는 시간급 최저임금액의 80%를 지급할 수 있습니다(같은 법 제5조 제2항, 같은 법 시행령 제3조).그러나 같은 법 시행령 제6조는 사용자는 고용노동부장관의 인가를 받아 ‘근로자의 정신 또는 신체의 장애가 당해 근로자를 종사시키고자 하는 업무의 수행에 직접적으로 현저한 지장을 주는 것이 명백하다고 인정되는 자’에 대하여는 최저임금의 적용을 제외할 수 있도록 규정하고 있습니다.고용노동부장관은 매년 3월 31일까지 근로자위원, 사용자위원, 공익위원 등으로 구성된 최저임금심의위원회에 최저임금에 관한 심의를 요청하여야 하고, 동 위원회에서는 근로자의 생계비, 유사근로자의 임금, 노동생산성 및 소득분배율 등을 고려하여 최저임금안을 심의하며, 심의위원회로부터 최저임금안을 제출받은 때에는 지체없이 사업의 종류별 최저임금안 및 적용사업의 범위를 고시하여야 하며, 매년 8월 5일까지 최저임금을 결정하여야 합니다(같은 법 제4조, 제5조, 제8조, 제9조, 같은 법 시행령 제7조).참고로 고용노동부에서 고시한 2016년 1월 1일부터 2016년 12월 31일 까지 적용되는 최저임금액을 보면 시간급은 6,030원으로 정하고 있습니다.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW8GdnRp_ak-"
      },
      "source": [
        "output 길이가 너무 긴 경우 학습이 제대로 되지 않을 수 있기 때문에, output의 토큰 수가 516 이하인 샘플만 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7chHJapf_N4n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "ceb9b3e0580b4eada5547b4020263d31",
            "2c9c6b58b67c40ca884220eaab1a1c49",
            "11f12cc4e8f14c16b1fc3372d1fa4fb0",
            "c23888845f9c408f8d2982477158d1a4",
            "1cd4ea7c96c7468384fdcc126c4efd4d",
            "c17114cdb05244a8b4c6f93e8ca30780",
            "bcaeb08be821402ba66c6861f42cc14c",
            "fdf7da85eafe442684f3dc37d14102ac",
            "337e23b5143e4f32b7a681a3cc70eb24",
            "5307331971654e5aad669cae22891c31",
            "0a5b4dc6be5e42f7b4e140b71be637aa"
          ]
        },
        "outputId": "a75852aa-9c87-41ab-8bf5-179995100298"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/14819 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ceb9b3e0580b4eada5547b4020263d31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10535"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# ✅ Step 2: output 길이 필터링 (토큰 길이 516 이하)\n",
        "def is_output_short(example):\n",
        "    tokenized = tokenizer(example[\"output\"], truncation=False, add_special_tokens=False)\n",
        "    return len(tokenized[\"input_ids\"]) <= 516 # True/False\n",
        "\n",
        "filtered_dataset = dataset.filter(is_output_short)\n",
        "len(filtered_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyYKfU1xuu8E",
        "outputId": "e78c8ba5-3d1d-4b2c-b1f6-06dd4164b373"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '저는 甲대학병원에서 전공의 과정을 밟고 있는 레지던트입니다. 그런데 몇 달 전부터 저에 대한 임금이 지급되지 않고 있어 병원측에 그 지급을 요구하였으나, 병원측은 레지던트 과정도 교육의 과정이므로 임금을 지급하지 않아도 되며 그 전에 얼마씩 지급한 것도 장학금 내지 생활비조로 병원측에서 호의적으로 지급한 것이었다고 말하고 있습니다. 저는 계속 이를 다투었다가는 장래에 좋지 않은 영향을 줄 것도 같아 어떻게 하여야 할지 모르겠습니다. 좋은 방법이 있는지요?',\n",
              " 'precedent': '',\n",
              " 'answer': '「근로기준법」제2조 제1항 1호에서는 ‘근로자’를 ‘직업의 종류와 관계없이 임금을 목적으로 사업이나 사업장에 근로를 제공하는 자를 말한다’라고 정의하고 있습니다.귀하의 경우 ‘전공의’과정은 ‘전문의’자격을 따기 위한 필수적인 수련과정에 해당한다는 점에서 과연 귀하가 근로기준법 소정의 ‘임금을 목적으로 근로를 제공하는 자’에 해당하는지 여부가 다투어질 수 있습니다.이에 관하여 판례는 “인턴 또는 레지던트 등 ‘수련의’, ‘전공의’의 경우에도 그들이 비록 전문의 시험자격취득을 위한 필수적인 수련과정에서 수련병원에 근로를 제공하였다고 하더라도 수련의, 전공의의 지위는 교과과정에서 정한 환자진료 등 피교육자적인 지위와 함께 병원에서 정한 진료계획에 따라 근로를 제공하고 그 대가로 임금을 지급받는 근로자로서의 지위를 아울러 가지고 있다 할 것이고, 또한 병원측의 지휘·감독아래 노무를 제공함으로써 실질적인 사용·종속관계에 있다고 할 것이므로 전공의는 병원경영자에 대한 관계에 있어서 근로기준법상의 근로자에 해당한다.”라고 하였습니다(대법원 1998. 4. 24. 선고 97다57672 판결, 2001. 3. 23. 선고 2000다39513 판결).따라서 귀하는 「근로기준법」의 보호를 받을 수 있으므로 임금 등의 지급을 구할 권리가 있고, 만일 이로 인해 어떠한 불이익한 처우를 받게 된 경우에는 역시 근로자의 지위에서 권리구제를 받을 수 있을 것입니다.                   ',\n",
              " 'instruction': '저는 甲대학병원에서 전공의 과정을 밟고 있는 레지던트입니다. 그런데 몇 달 전부터 저에 대한 임금이 지급되지 않고 있어 병원측에 그 지급을 요구하였으나, 병원측은 레지던트 과정도 교육의 과정이므로 임금을 지급하지 않아도 되며 그 전에 얼마씩 지급한 것도 장학금 내지 생활비조로 병원측에서 호의적으로 지급한 것이었다고 말하고 있습니다. 저는 계속 이를 다투었다가는 장래에 좋지 않은 영향을 줄 것도 같아 어떻게 하여야 할지 모르겠습니다. 좋은 방법이 있는지요?',\n",
              " 'output': '「근로기준법」제2조 제1항 1호에서는 ‘근로자’를 ‘직업의 종류와 관계없이 임금을 목적으로 사업이나 사업장에 근로를 제공하는 자를 말한다’라고 정의하고 있습니다.귀하의 경우 ‘전공의’과정은 ‘전문의’자격을 따기 위한 필수적인 수련과정에 해당한다는 점에서 과연 귀하가 근로기준법 소정의 ‘임금을 목적으로 근로를 제공하는 자’에 해당하는지 여부가 다투어질 수 있습니다.이에 관하여 판례는 “인턴 또는 레지던트 등 ‘수련의’, ‘전공의’의 경우에도 그들이 비록 전문의 시험자격취득을 위한 필수적인 수련과정에서 수련병원에 근로를 제공하였다고 하더라도 수련의, 전공의의 지위는 교과과정에서 정한 환자진료 등 피교육자적인 지위와 함께 병원에서 정한 진료계획에 따라 근로를 제공하고 그 대가로 임금을 지급받는 근로자로서의 지위를 아울러 가지고 있다 할 것이고, 또한 병원측의 지휘·감독아래 노무를 제공함으로써 실질적인 사용·종속관계에 있다고 할 것이므로 전공의는 병원경영자에 대한 관계에 있어서 근로기준법상의 근로자에 해당한다.”라고 하였습니다(대법원 1998. 4. 24. 선고 97다57672 판결, 2001. 3. 23. 선고 2000다39513 판결).따라서 귀하는 「근로기준법」의 보호를 받을 수 있으므로 임금 등의 지급을 구할 권리가 있고, 만일 이로 인해 어떠한 불이익한 처우를 받게 된 경우에는 역시 근로자의 지위에서 권리구제를 받을 수 있을 것입니다.                   '}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853,
          "referenced_widgets": [
            "2289d324cffa480792834f8d654020c6",
            "bd9ec9cbd41e4d249ca6eb5482c1fe57",
            "de0b22e262624f4493893b1cd1da25c0",
            "220f1eb8151d416a9c7aec8b343e50f8",
            "6783886927c640c0949f21e1f8a297fd",
            "841334345ed94936a7696b740c8a8989",
            "748728f90b4f4d009b73e9da8c347837",
            "e6759a358e6e47da97b030ee2c0a3ade",
            "2621ccf612394064835c2726279c1bfc",
            "36ba45454a3f4f2eb73de5e14a8de12a",
            "3dfc3cdb99f7432db4a7f0722953d7bc"
          ]
        },
        "id": "pF7C8nl3_aHD",
        "outputId": "d34539c9-d339-473e-fb73-8ff381eb2254"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10535 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2289d324cffa480792834f8d654020c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 필터링 후 샘플 수: 10535\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '저는 甲대학병원에서 전공의 과정을 밟고 있는 레지던트입니다. 그런데 몇 달 전부터 저에 대한 임금이 지급되지 않고 있어 병원측에 그 지급을 요구하였으나, 병원측은 레지던트 과정도 교육의 과정이므로 임금을 지급하지 않아도 되며 그 전에 얼마씩 지급한 것도 장학금 내지 생활비조로 병원측에서 호의적으로 지급한 것이었다고 말하고 있습니다. 저는 계속 이를 다투었다가는 장래에 좋지 않은 영향을 줄 것도 같아 어떻게 하여야 할지 모르겠습니다. 좋은 방법이 있는지요?',\n",
              " 'precedent': '',\n",
              " 'answer': '「근로기준법」제2조 제1항 1호에서는 ‘근로자’를 ‘직업의 종류와 관계없이 임금을 목적으로 사업이나 사업장에 근로를 제공하는 자를 말한다’라고 정의하고 있습니다.귀하의 경우 ‘전공의’과정은 ‘전문의’자격을 따기 위한 필수적인 수련과정에 해당한다는 점에서 과연 귀하가 근로기준법 소정의 ‘임금을 목적으로 근로를 제공하는 자’에 해당하는지 여부가 다투어질 수 있습니다.이에 관하여 판례는 “인턴 또는 레지던트 등 ‘수련의’, ‘전공의’의 경우에도 그들이 비록 전문의 시험자격취득을 위한 필수적인 수련과정에서 수련병원에 근로를 제공하였다고 하더라도 수련의, 전공의의 지위는 교과과정에서 정한 환자진료 등 피교육자적인 지위와 함께 병원에서 정한 진료계획에 따라 근로를 제공하고 그 대가로 임금을 지급받는 근로자로서의 지위를 아울러 가지고 있다 할 것이고, 또한 병원측의 지휘·감독아래 노무를 제공함으로써 실질적인 사용·종속관계에 있다고 할 것이므로 전공의는 병원경영자에 대한 관계에 있어서 근로기준법상의 근로자에 해당한다.”라고 하였습니다(대법원 1998. 4. 24. 선고 97다57672 판결, 2001. 3. 23. 선고 2000다39513 판결).따라서 귀하는 「근로기준법」의 보호를 받을 수 있으므로 임금 등의 지급을 구할 권리가 있고, 만일 이로 인해 어떠한 불이익한 처우를 받게 된 경우에는 역시 근로자의 지위에서 권리구제를 받을 수 있을 것입니다.                   ',\n",
              " 'instruction': '저는 甲대학병원에서 전공의 과정을 밟고 있는 레지던트입니다. 그런데 몇 달 전부터 저에 대한 임금이 지급되지 않고 있어 병원측에 그 지급을 요구하였으나, 병원측은 레지던트 과정도 교육의 과정이므로 임금을 지급하지 않아도 되며 그 전에 얼마씩 지급한 것도 장학금 내지 생활비조로 병원측에서 호의적으로 지급한 것이었다고 말하고 있습니다. 저는 계속 이를 다투었다가는 장래에 좋지 않은 영향을 줄 것도 같아 어떻게 하여야 할지 모르겠습니다. 좋은 방법이 있는지요?',\n",
              " 'output': '「근로기준법」제2조 제1항 1호에서는 ‘근로자’를 ‘직업의 종류와 관계없이 임금을 목적으로 사업이나 사업장에 근로를 제공하는 자를 말한다’라고 정의하고 있습니다.귀하의 경우 ‘전공의’과정은 ‘전문의’자격을 따기 위한 필수적인 수련과정에 해당한다는 점에서 과연 귀하가 근로기준법 소정의 ‘임금을 목적으로 근로를 제공하는 자’에 해당하는지 여부가 다투어질 수 있습니다.이에 관하여 판례는 “인턴 또는 레지던트 등 ‘수련의’, ‘전공의’의 경우에도 그들이 비록 전문의 시험자격취득을 위한 필수적인 수련과정에서 수련병원에 근로를 제공하였다고 하더라도 수련의, 전공의의 지위는 교과과정에서 정한 환자진료 등 피교육자적인 지위와 함께 병원에서 정한 진료계획에 따라 근로를 제공하고 그 대가로 임금을 지급받는 근로자로서의 지위를 아울러 가지고 있다 할 것이고, 또한 병원측의 지휘·감독아래 노무를 제공함으로써 실질적인 사용·종속관계에 있다고 할 것이므로 전공의는 병원경영자에 대한 관계에 있어서 근로기준법상의 근로자에 해당한다.”라고 하였습니다(대법원 1998. 4. 24. 선고 97다57672 판결, 2001. 3. 23. 선고 2000다39513 판결).따라서 귀하는 「근로기준법」의 보호를 받을 수 있으므로 임금 등의 지급을 구할 권리가 있고, 만일 이로 인해 어떠한 불이익한 처우를 받게 된 경우에는 역시 근로자의 지위에서 권리구제를 받을 수 있을 것입니다.                   ',\n",
              " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n저는 甲대학병원에서 전공의 과정을 밟고 있는 레지던트입니다. 그런데 몇 달 전부터 저에 대한 임금이 지급되지 않고 있어 병원측에 그 지급을 요구하였으나, 병원측은 레지던트 과정도 교육의 과정이므로 임금을 지급하지 않아도 되며 그 전에 얼마씩 지급한 것도 장학금 내지 생활비조로 병원측에서 호의적으로 지급한 것이었다고 말하고 있습니다. 저는 계속 이를 다투었다가는 장래에 좋지 않은 영향을 줄 것도 같아 어떻게 하여야 할지 모르겠습니다. 좋은 방법이 있는지요?\\n\\n### Response:\\n「근로기준법」제2조 제1항 1호에서는 ‘근로자’를 ‘직업의 종류와 관계없이 임금을 목적으로 사업이나 사업장에 근로를 제공하는 자를 말한다’라고 정의하고 있습니다.귀하의 경우 ‘전공의’과정은 ‘전문의’자격을 따기 위한 필수적인 수련과정에 해당한다는 점에서 과연 귀하가 근로기준법 소정의 ‘임금을 목적으로 근로를 제공하는 자’에 해당하는지 여부가 다투어질 수 있습니다.이에 관하여 판례는 “인턴 또는 레지던트 등 ‘수련의’, ‘전공의’의 경우에도 그들이 비록 전문의 시험자격취득을 위한 필수적인 수련과정에서 수련병원에 근로를 제공하였다고 하더라도 수련의, 전공의의 지위는 교과과정에서 정한 환자진료 등 피교육자적인 지위와 함께 병원에서 정한 진료계획에 따라 근로를 제공하고 그 대가로 임금을 지급받는 근로자로서의 지위를 아울러 가지고 있다 할 것이고, 또한 병원측의 지휘·감독아래 노무를 제공함으로써 실질적인 사용·종속관계에 있다고 할 것이므로 전공의는 병원경영자에 대한 관계에 있어서 근로기준법상의 근로자에 해당한다.”라고 하였습니다(대법원 1998. 4. 24. 선고 97다57672 판결, 2001. 3. 23. 선고 2000다39513 판결).따라서 귀하는 「근로기준법」의 보호를 받을 수 있으므로 임금 등의 지급을 구할 권리가 있고, 만일 이로 인해 어떠한 불이익한 처우를 받게 된 경우에는 역시 근로자의 지위에서 권리구제를 받을 수 있을 것입니다.                   <|eot_id|>'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# ✅ Step 3: text 필드 생성\n",
        "def formatting_prompts_func(examples):\n",
        "  ## examples: dict\n",
        "    instructions = examples[\"instruction\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\": texts }\n",
        "\n",
        "formatted_dataset = filtered_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# ✅ 결과 확인\n",
        "print(f\"📊 필터링 후 샘플 수: {len(formatted_dataset)}\")\n",
        "formatted_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVTYvb45-RyD",
        "outputId": "efba7b7f-aec7-48dc-a446-3a40c98842af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "저는 甲대학병원에서 전공의 과정을 밟고 있는 레지던트입니다. 그런데 몇 달 전부터 저에 대한 임금이 지급되지 않고 있어 병원측에 그 지급을 요구하였으나, 병원측은 레지던트 과정도 교육의 과정이므로 임금을 지급하지 않아도 되며 그 전에 얼마씩 지급한 것도 장학금 내지 생활비조로 병원측에서 호의적으로 지급한 것이었다고 말하고 있습니다. 저는 계속 이를 다투었다가는 장래에 좋지 않은 영향을 줄 것도 같아 어떻게 하여야 할지 모르겠습니다. 좋은 방법이 있는지요?\n",
            "\n",
            "### Response:\n",
            "「근로기준법」제2조 제1항 1호에서는 ‘근로자’를 ‘직업의 종류와 관계없이 임금을 목적으로 사업이나 사업장에 근로를 제공하는 자를 말한다’라고 정의하고 있습니다.귀하의 경우 ‘전공의’과정은 ‘전문의’자격을 따기 위한 필수적인 수련과정에 해당한다는 점에서 과연 귀하가 근로기준법 소정의 ‘임금을 목적으로 근로를 제공하는 자’에 해당하는지 여부가 다투어질 수 있습니다.이에 관하여 판례는 “인턴 또는 레지던트 등 ‘수련의’, ‘전공의’의 경우에도 그들이 비록 전문의 시험자격취득을 위한 필수적인 수련과정에서 수련병원에 근로를 제공하였다고 하더라도 수련의, 전공의의 지위는 교과과정에서 정한 환자진료 등 피교육자적인 지위와 함께 병원에서 정한 진료계획에 따라 근로를 제공하고 그 대가로 임금을 지급받는 근로자로서의 지위를 아울러 가지고 있다 할 것이고, 또한 병원측의 지휘·감독아래 노무를 제공함으로써 실질적인 사용·종속관계에 있다고 할 것이므로 전공의는 병원경영자에 대한 관계에 있어서 근로기준법상의 근로자에 해당한다.”라고 하였습니다(대법원 1998. 4. 24. 선고 97다57672 판결, 2001. 3. 23. 선고 2000다39513 판결).따라서 귀하는 「근로기준법」의 보호를 받을 수 있으므로 임금 등의 지급을 구할 권리가 있고, 만일 이로 인해 어떠한 불이익한 처우를 받게 된 경우에는 역시 근로자의 지위에서 권리구제를 받을 수 있을 것입니다.                   <|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(formatted_dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHy0Vm_zgYQx"
      },
      "source": [
        "### (2) LoRA 설정 및 모델 준비(QLoRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsNV4Jg4gSS-"
      },
      "source": [
        "이 단계에서는 Unsloth를 활용해 사전 로드한 LLM 모델에 LoRA 설정만 적용하여, 추후 학습이 가능한 상태로 준비합니다.\n",
        "\n",
        "Unsloth의 FastLanguageModel.get_peft_model() 함수를 사용하면 QLoRA 방식으로 4bit 양자화된 모델에 LoRA Adapter를 얹어 빠르게 설정할 수 있습니다.\n",
        "\n",
        "\n",
        "- FastLanguageModel : Unsloth의 핵심 클래스. 빠른 로딩 및 LoRA 적용 지원\n",
        "- load_in_4bit=True : GPU 메모리 절약을 위한 4bit 양자화 적용 -> QLoRA\n",
        "- LoRA Adapter : 파인튜닝 시 일부 레이어만 학습하여 효율 극대화\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**QLoRA**(Quantized LoRA)는 기존 LoRA에 **4-bit 양자화(Quantization)** 기법을 결합해,  \n",
        "모델 전체를 4비트로 축소(quantize)한 뒤 필요한 LoRA 어댑터 부분만 학습하는 방식입니다.  \n",
        "이를 통해 **VRAM 사용량**을 대폭 줄이면서도 LoRA의 장점을 그대로 살릴 수 있습니다.\n",
        "\n",
        "1) 기존 LoRA vs. QLoRA\n",
        "- **LoRA**  \n",
        "  - 원본 모델 전체를 FP16(16-bit 부동소수점)으로 유지  \n",
        "  - VRAM 절감 효과가 풀 파인튜닝 대비 크지만, 대규모 모델(예: 수십억 파라미터 이상)에 대해 여전히 **GPU 메모리 부담**이 있을 수 있음\n",
        "- **QLoRA**  \n",
        "  - 모델 가중치를 **4-bit**로 양자화하여 로드  \n",
        "  - LoRA의 저랭크 행렬(어댑터)만 **FP16** 혹은 FP32로 학습  \n",
        "  - 원본 가중치는 거의 수정되지 않으며, 4비트로 메모리를 최소화  \n",
        "  - 특히 **A100, 3090, 4090** 등 GPU를 사용 시 **LoRA보다도 훨씬 적은 VRAM**으로 훈련 가능\n",
        "\n",
        "2) 장단점\n",
        "- **장점**  \n",
        "  - **메모리 사용량**이 현저히 감소해, 제한된 GPU 환경에서도 대형 모델 튜닝 가능  \n",
        "  - LoRA와 결합으로, **높은 성능**을 유지하면서도 **효율적**인 미세튜닝  \n",
        "  - GPU 비용을 절감하고, **더 큰 배치 사이즈**로 학습할 수도 있음\n",
        "- **단점**  \n",
        "  - 4-bit 양자화로 인해 **수치 정밀도**가 떨어지므로, 모델이 **아주 세밀한 표현**을 다룰 때 성능이 조금 희생될 수 있음  \n",
        "  - 일부 상황(특히 이미 작은 모델)에선 양자화로 이득이 크지 않을 수도 있음  \n",
        "  - 특정 연산(예: 커스텀 CUDA 커널 지원)이 필요하므로, **bitsandbytes 등 추가 라이브러리**가 있어야 함\n",
        "\n",
        "\n",
        "  ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "3b7af97ba1ca4c67843e35d38c164f6b",
            "a08370d9e1554837966bd7693e826be2",
            "67f5a1803b79436a9caffe19ea2fc104",
            "25d1caa30f314443b66e10f4b358f020",
            "53c5add6602c4c68a8946468e1306d55",
            "76ddc44dabdd4f46b8afba98552a83ad",
            "43a959afe63f4d21acb0f9f3f09301a8",
            "27d50be7325946e99b94e0c95d35a38b",
            "1182f737d0694499a9875d0f72b67bc0",
            "d4297597d3ba4d90bc87d3d9d05aa9fe",
            "82a2c5a1ce434de39c057fa98619b0e2"
          ]
        },
        "id": "KDUjqkiSWTlC",
        "outputId": "24df5d58-43c6-48a4-9b74-4183d72e09ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b7af97ba1ca4c67843e35d38c164f6b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\",  # ✅ 사용 모델\n",
        "    max_seq_length = 2048,      # ✅ 권장 설정\n",
        "    dtype = None,               # ✅ float16 or bfloat16 자동 선택\n",
        "    load_in_4bit = True         # ✅ 4bit QLoRA 양자화 로딩 활성화\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8VsnWGGKPYZ"
      },
      "source": [
        "- FastLanguageModel.get_peft_model\n",
        "    - r : LoRA의 랭크 값. 클수록 성능 좋지만 메모리 사용 증가 (보통 8~64)\n",
        "    - lora_alpha : 학습 속도와 안정성 제어하는 스케일링 계수\n",
        "    - lora_dropout : LoRA 레이어에만 적용되는 드롭아웃 확률 (0.05 권장)\n",
        "    - bias : \"none\"으로 설정 시 LoRA matrix만 학습 (보통 이 값 사용)\n",
        "    - target_modules : LoRA를 적용할 Linear 계층 명시 (q_proj, v_proj 등)\n",
        "    - use_gradient_checkpointing : 메모리 절약 위한 역전파 시 체크포인팅 사용\n",
        "    - use_rslora : Rescaled LoRA 사용 여부 (False로 설정)\n",
        "    - loftq_config : optional, QLoRA에 LoFTQ 기법 적용할 경우 지정\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDtnmAc2gW57",
        "outputId": "1f5287b8-ac94-426a-94ba-f42872201b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# 이미 로드한 모델과 토크나이저에 LoRA 설정만 적용\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # LoRA 랭크 설정. 8, 16, 32, 64, 128 권장. r 값이 클수록 모델이 더 많은 정보를 학습할 수 있지만, 너무 크면 메모리를 많이 사용\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # PEFT 적용할 모듈 목록. 모델의 특정 부분(모듈)에만 학습\n",
        "    lora_alpha = 16,        # LoRA 알파 설정 LoRA라는 기술이 얼마나 강하게 작용할지 조절\n",
        "    lora_dropout = 0,       # LoRA 드롭아웃 설정. 0으로 최적화\n",
        "    bias = \"none\",          # 바이어스 설정. \"none\"으로 최적화\n",
        "\n",
        "    # \"unsloth\" 사용 시 VRAM 절약 및 배치 사이즈 2배 증가\n",
        "    # 학습할 때 메모리를 절약하는 방법을 사용하는 설정\n",
        "    use_gradient_checkpointing = \"unsloth\",  # 매우 긴 컨텍스트를 위해 \"unsloth\" 설정\n",
        "    random_state = 42,    # 랜덤 시드 설정\n",
        "    use_rslora = False,     # 랭크 안정화 LoRA 사용 여부\n",
        "    loftq_config = None,    # LoftQ 설정 (사용하지 않음)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28VO1bubga60"
      },
      "source": [
        "### (3) LoRA(QLoRA)를 이용한 모델 파인튜닝\n",
        "\n",
        "\n",
        "\n",
        "이 단계에서는 SFTTrainer를 활용하여 실제로 LoRA(QLoRA) 기반 파인튜닝을 수행합니다.\n",
        "\n",
        "Unsloth와 TRL을 활용하여 단일턴 정보 제공형 챗봇에 맞게 구성되어 있으며,\n",
        "모든 학습은 4bit로 양자화된 모델의 LoRA Adapter 파라미터만 업데이트합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqeTjBDdLHU_"
      },
      "source": [
        "- 주요 학습 인자 설명    \n",
        "    - per_device_train_batch_size: 각 디바이스(GPU) 당 학습 배치 크기\n",
        "    - gradient_accumulation_steps: 메모리 제한 시 그레이디언트 누적 단계 수\n",
        "    - max_steps: 전체 학습 반복 횟수 (데모는 100, 실전은 수천 이상 설정)\n",
        "    - learning_rate: 파인튜닝 시 모델 가중치 업데이트 속도\n",
        "    - fp16, bf16: GPU 환경에 맞춰 혼합 정밀도 학습 설정\n",
        "    - optim: 양자화된 모델에 최적화된 옵티마이저 사용 (adamw_8bit)\n",
        "    - output_dir: 학습 결과(모델) 저장 경로\n",
        "    - packing: 여러 샘플을 하나로 패킹할지 여부 (False 권장)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "293907ade9f4442fadbf74f808a47897",
            "5b76bcb6619e485f8f30893750863cbf",
            "1a63df4436f740e0ba0349e6fcd637f3",
            "158aaf949454451db4225a20ab5173ed",
            "3c0bd1e366e14ed58a6d3331d75f5005",
            "9fba71745e5a428183045dd6b75a21a4",
            "d9720e71b2fb4c3485795f45f450e3fb",
            "236defe62a344367ad123a818689e703",
            "c0c03a87a03d44629b474a26b70bb489",
            "98ea165228e44602954829659c21f66a",
            "99900f9b273f4942a99bd6a90ba54dc0"
          ]
        },
        "id": "fyA3E-Z1B0Yw",
        "outputId": "14eca932-e50c-464f-f451-4ea84ddefed5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/10535 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "293907ade9f4442fadbf74f808a47897"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported  # BFloat16 지원 여부 확인 함수 임포트\n",
        "\n",
        "max_seq_length = 1024\n",
        "\n",
        "# SFTTrainer를 사용하여 모델 학습 설정\n",
        "# SFTTrainer 인스턴스 생성\n",
        "trainer = SFTTrainer(\n",
        "    model = model,                           # 학습할 모델\n",
        "    tokenizer = tokenizer,                   # 사용할 토크나이저\n",
        "    train_dataset = formatted_dataset,                 # 학습할 데이터셋 ★★★★★★★★\n",
        "    dataset_text_field = \"text\",             # 데이터셋의 텍스트 필드 이름 ★★★★★★★★\n",
        "    max_seq_length = max_seq_length,         # 최대 시퀀스 길이\n",
        "    dataset_num_proc = 2,                    # 데이터셋 전처리에 사용할 프로세스 수 cpu\n",
        "    packing = False,                         # 짧은 시퀀스의 경우 packing을 비활성화 (학습 속도 5배 향상 가능)\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,     # 디바이스 당 배치 사이즈\n",
        "        gradient_accumulation_steps = 4,     # 그래디언트 누적 단계 수\n",
        "        warmup_steps = 5,                     # 워밍업 스텝 수\n",
        "        # num_train_epochs = 1,               # 전체 학습 에폭 수 설정 가능\n",
        "        max_steps = 60,                       # 최대 학습 스텝 수(weight 업데이트 수)\n",
        "        learning_rate = 2e-4,                 # 학습률\n",
        "        fp16 = not is_bfloat16_supported(),   # BFloat16 지원 여부에 따라 FP16 사용\n",
        "        bf16 = is_bfloat16_supported(),       # BFloat16 사용 여부\n",
        "        logging_steps = 1,                    # 로깅 빈도\n",
        "        optim = \"adamw_8bit\",                  # 옵티마이저 설정 (8비트 AdamW) 경량화된 adamw⭐️\n",
        "        weight_decay = 0.01,                  # 가중치 감쇠\n",
        "        lr_scheduler_type = \"linear\",         # 학습률 스케줄러 타입\n",
        "        seed = 3407,                           # 랜덤 시드 설정\n",
        "        output_dir = \"outputs\",                # 출력 디렉토리\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n14mM8jbhhdg"
      },
      "source": [
        "학습 실행\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d72n6BjZiz9v",
        "outputId": "3100ae24-4e75-41ee-945e-a5073a814809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 10,535 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 12:20, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.077100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.218900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.854500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.781100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.172500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.705600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.777600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.390200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.167200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.214400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.296600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.346800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.360500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.427800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.313500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.973700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.907000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.985600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.388300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.838500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.042600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.894200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.726400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.944600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.134400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.895200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.953600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.219900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.873300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.107600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.787300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.838500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.800300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.923500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.009300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.031000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.883300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.987600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.814900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.790100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.178900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.562100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.620300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.149200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.036000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.800300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.986100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.821200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.845800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.774100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.997500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.918800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.769500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGkw81zyV4JM",
        "outputId": "b771833f-51c1-4b03-c414-7096359f6a04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Qlora_model/tokenizer_config.json',\n",
              " 'Qlora_model/special_tokens_map.json',\n",
              " 'Qlora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# 모델 저장 로컬폴더에다가 저장하는 방식\n",
        "model.save_pretrained(\"Qlora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"Qlora_model\")\n",
        "\n",
        "# 이 이외에 허깅페이스나 다른 hub에 push해서 저장하는 방법이 있음\n",
        "# 다만, 업로드 속도와 다운로드 속도를 고려해야함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iWMFRxCY1TY"
      },
      "source": [
        "🔍 참고: 과적합(Overfitting) & 과소적합(Underfitting) 방지 팁\n",
        "\n",
        "과적합 (Overfitting)\n",
        "- 학습 데이터에 너무 특화되어 새로운 입력에 일반화되지 못하는 문제\n",
        "- 해결책:\n",
        "    - 학습률 감소\n",
        "    - 학습 epoch 수 줄이기\n",
        "    - ShareGPT와 같은 일반 데이터셋과 혼합\n",
        "    - Dropout 비율 증가 (규제화 강화)\n",
        "\n",
        "과소적합 (Underfitting)\n",
        "- 모델이 도메인 특화된 지식을 충분히 학습하지 못하고 기본 모델 수준에 머무는 경우 (드물게 발생)\n",
        "- 해결책:\n",
        "    - 학습률 증가\n",
        "    - 더 많은 epoch 학습\n",
        "    - 더 도메인 관련성이 높은 데이터셋 사용\n",
        "\n",
        "\n",
        "\n",
        "🧪 참고: Fine-tuning은 정해진 \"최선의 방법\"은 없으며, 실험을 통해 최적 조합을 찾는 것이 핵심입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7FQ5xiHga3-"
      },
      "source": [
        "### (4) 파인튜닝된 모델 평가 (Inference)\n",
        "\n",
        "\n",
        "이 단계에서는 우리가 학습한 한국어 법률 챗봇 LoRA 모델이 입력에 대해 얼마나 자연스럽게 응답하는지 확인합니다.\n",
        "\n",
        "\n",
        "Unsloth의 FastLanguageModel.from_pretrained()를 사용하면 2배 빠른 추론 속도로 저장된 LoRA 모델을 로드할 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "1fbfbebb192c480d96f5b80e2c301e28",
            "5ba7925fd80f4173b82a0583283a159d",
            "6f93357cb75a4780b3ddad52c4556551",
            "35d51be188304889a515886b5aac20f1",
            "e278bce3159d4201a1e38a112dfd55f5",
            "aad9806d38a041c185c9ff9ac15cbda8",
            "39e3aa76b0d8403b9de1e8d4b968cfdc",
            "404f6656a9354c76861b6e97289bc4c5",
            "8f9fac0640b3427b99c790ee94840b05",
            "0383d0fad41848ddbd52b9a1c43d043b",
            "1c3ff582e28443d5b98d52367f520cdd"
          ]
        },
        "id": "iSA_LtFhV-b4",
        "outputId": "e47a73df-ea90-4183-80f3-d7b2ffaec31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fbfbebb192c480d96f5b80e2c301e28"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 저장된 경로 지정\n",
        "save_directory = \"Qlora_model\"\n",
        "\n",
        "# 모델과 토크나이저 불러오기\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_directory,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,  # 양자화 옵션을 동일하게 설정\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W0Sm1P5WAz3",
        "outputId": "c486f85f-62b1-41ac-fcd0-c749f7ea6b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "유튜브 제목을 3개만 만들어줘. 주제: ai의 미래\n",
            "\n",
            "### Response:\n",
            "1. AI가 사람을 대체할 수 있을까?\n",
            "2. AI가 인간의 인지 능력을 뛰어넘을 수 있을까?\n",
            "3. AI가 인간의 삶을 어떻게 변화시킬 수 있을까?<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# 추론해보기\n",
        "FastLanguageModel.for_inference(model)  # 네이티브 2배 빠른 추론 활성화\n",
        "\n",
        "# 추론을 위한 입력 준비\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"유튜브 제목을 3개만 만들어줘. 주제: ai의 미래\", # 인스트럭션 (명령어)\n",
        "        \"\", # 출력 - 생성할 답변을 비워둠\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")  # 텐서를 PyTorch 형식으로 변환하고 GPU로 이동\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)  # 토크나이저를 사용하여 스트리머 초기화\n",
        "\n",
        "# 모델을 사용하여 텍스트 생성 및 스트리밍 출력\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128, eos_token_id=tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"))  # 특정 토큰이 나오면 종료)  # 최대 128개의 새로운 토큰 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SVKOJYprPg_"
      },
      "source": [
        "약간의 학습을 진행했음에도 이전보다 답변 품질이 향상된 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuwMSz3JZvaJ"
      },
      "source": [
        "이제,\n",
        "\n",
        "각 모델의 성능을 정량적으로 비교하기 위해 챗봇 평가 매트릭스(BLEU, ROUGE, Distinct-N, GPT Score 등)를 통해 평가를 진행하겠습니다.\n",
        "\n",
        "\n",
        "\n",
        "1️⃣ BLEU (Bilingual Evaluation Understudy)\n",
        "- 번역 품질 평가 지표지만, 챗봇 응답이 정답과 얼마나 일치하는지 측정하는 데 활용 가능.\n",
        "- 단어 n-gram의 일치율을 기반으로 평가 (BLEU-1, BLEU-2 등).\n",
        "\n",
        "2️⃣ ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "- 문서 요약 평가에 자주 쓰이지만, 챗봇 응답의 요약 품질을 평가하는 데도 활용 가능.\n",
        "- ROUGE-1 (단어 기반), ROUGE-2 (2-gram 기반), ROUGE-L (Longest Common Subsequence 기반) 사용 가능.\n",
        "\n",
        "3️⃣ Distinct-N Score\n",
        "- 생성된 응답의 다양성을 평가하는 지표.\n",
        "- Distinct-1 (고유한 단어 비율), Distinct-2 (고유한 2-gram 비율)를 측정하여 반복적인 응답을 방지하는 능력을 확인.\n",
        "\n",
        "4️⃣ GPT-Score / BERTScore\n",
        "- GPT-4 또는 BERT 기반 평가 모델을 활용하여 챗봇 응답의 자연스러움과 의미적 유사도를 비교.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK0Na9albreF",
        "outputId": "b11d1400-2518-474a-d2ab-8d35c01bae23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mj6J8MGaOiC",
        "outputId": "8d132c9d-2d5d-41f4-b4e2-00e2adf00e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 [1000개 샘플 평균 평가 결과]\n",
            "BLEU-1 평균: 0.0520\n",
            "BLEU-2 평균: 0.0255\n",
            "-------------------------\n",
            "ROUGE-1 평균: 0.0939\n",
            "ROUGE-2 평균: 0.0235\n",
            "ROUGE-L 평균: 0.0871\n",
            "-------------------------\n",
            "Distinct-1 평균: 0.7418\n",
            "Distinct-2 평균: 0.8750\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "eval_size = 10\n",
        "\n",
        "bleu1_list, bleu2_list = [], []\n",
        "rouge1_list, rouge2_list, rougel_list = [], [], []\n",
        "distinct1_list, distinct2_list = [], []\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "for i in range(eval_size):\n",
        "    reference = formatted_dataset[i][\"output\"].strip()\n",
        "    instruction = formatted_dataset[i][\"instruction\"].strip()\n",
        "\n",
        "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if \"### Response:\" in decoded:\n",
        "        generated = decoded.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        generated = decoded.strip()\n",
        "\n",
        "    try:\n",
        "        bleu1 = sentence_bleu([reference.split()], generated.split(), weights=(1, 0, 0, 0))\n",
        "        bleu2 = sentence_bleu([reference.split()], generated.split(), weights=(0.5, 0.5, 0, 0))\n",
        "        rouge_scores = rouge.get_scores(generated, reference)[0]\n",
        "        tokens = generated.split()\n",
        "        bigrams = list(zip(tokens, tokens[1:]))\n",
        "        distinct1 = len(set(tokens)) / max(len(tokens), 1)\n",
        "        distinct2 = len(set(bigrams)) / max(len(bigrams), 1)\n",
        "\n",
        "        bleu1_list.append(bleu1)\n",
        "        bleu2_list.append(bleu2)\n",
        "        rouge1_list.append(rouge_scores[\"rouge-1\"][\"f\"])\n",
        "        rouge2_list.append(rouge_scores[\"rouge-2\"][\"f\"])\n",
        "        rougel_list.append(rouge_scores[\"rouge-l\"][\"f\"])\n",
        "        distinct1_list.append(distinct1)\n",
        "        distinct2_list.append(distinct2)\n",
        "    except:\n",
        "        print(f\"⚠️ {i}번째 샘플 평가 실패\")\n",
        "\n",
        "print(\"📊 [1000개 샘플 평균 평가 결과]\")\n",
        "print(f\"BLEU-1 평균: {sum(bleu1_list)/len(bleu1_list):.4f}\")\n",
        "print(f\"BLEU-2 평균: {sum(bleu2_list)/len(bleu2_list):.4f}\")\n",
        "print('-------------------------')\n",
        "print(f\"ROUGE-1 평균: {sum(rouge1_list)/len(rouge1_list):.4f}\")\n",
        "print(f\"ROUGE-2 평균: {sum(rouge2_list)/len(rouge2_list):.4f}\")\n",
        "print(f\"ROUGE-L 평균: {sum(rougel_list)/len(rougel_list):.4f}\")\n",
        "print('-------------------------')\n",
        "print(f\"Distinct-1 평균: {sum(distinct1_list)/len(distinct1_list):.4f}\")\n",
        "print(f\"Distinct-2 평균: {sum(distinct2_list)/len(distinct2_list):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDz50zEYZHBF"
      },
      "source": [
        "### (5) DoRA: Weight-Decomposed LoRA\n",
        "\n",
        "\n",
        "\n",
        "DoRA(Weight-Decomposed LoRA)는 기존 LoRA의 성능 한계를 극복하기 위해 제안된 새로운 PEFT(파라미터 효율적 미세조정) 방식입니다.\n",
        "\n",
        "\n",
        "LoRA는 간단하고 효율적이지만, 일부 경우 정확도가 감소하거나 훈련이 불안정해질 수 있습니다.\n",
        "\n",
        "DoRA는 이러한 한계를 해결하면서 더 적은 파라미터로도 더 높은 성능을 보일 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en1zhxJgZNHB"
      },
      "source": [
        "📌 DoRA란?\n",
        "\n",
        "기존 LoRA는 가중치를 다음과 같이 덧셈 방식으로 조정합니다:\n",
        "```\n",
        "    W = W₀ + ΔW\n",
        "```\n",
        "\n",
        "\n",
        "- W₀: 원래 모델의 가중치\n",
        "- ΔW: 학습된 LoRA 저랭크 어댑터 가중치\n",
        "\n",
        "그러나 ΔW는 W₀의 크기(norm) 또는 방향(direction) 을 고려하지 않기 때문에\n",
        "과도한 튜닝이나 일반화 실패 등의 문제가 발생할 수 있습니다.\n",
        "\n",
        "\n",
        "DoRA는 이를 해결하기 위해, 가중치를 곱셈 형태로 재구성합니다:\n",
        "\n",
        "```\n",
        "    W = s × d\n",
        "```\n",
        "\n",
        "- s: 스케일(norm), 고정된 값 (학습 ❌)\n",
        "- d: 방향성(direction), 학습되는 파라미터\n",
        "\n",
        "\n",
        "즉, DoRA는 \"방향만 학습\"하고 \"크기는 고정\"하는 구조를 통해\n",
        "보다 안정적인 학습과 정규화 효과를 동시에 얻을 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aADWMNKasLR"
      },
      "source": [
        "Unsloth에서 DoRA는 쉽게 설정이 가능합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'런타임 - 세션 다시 시작' 후 실행"
      ],
      "metadata": {
        "id": "gLjaiSxylHiD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "HCt7NwpqalM7",
        "outputId": "a9e33fe6-0892-4b88-f6d9-a7ad83cdc30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-361811b12176>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# ✅ 사용 모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# ✅ 권장 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4262\u001b[0;31m                 \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4264\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\",  # ✅ 사용 모델\n",
        "    max_seq_length = 2048,      # ✅ 권장 설정\n",
        "    dtype = None,               # ✅ float16 or bfloat16 자동 선택\n",
        "    load_in_4bit = True         # ✅ 4bit QLoRA 양자화 로딩 비활성화\n",
        ")\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.0,\n",
        "    bias = \"none\",\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    use_dora = True,  # ✅ DoRA 활성화\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lor-gn7mL8Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hevP5ja2L8Hf"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# ✅ 데이터셋 로드\n",
        "dataset = load_dataset(\"jihye-moon/LawQA-Ko\", split=\"train\")\n",
        "\n",
        "# ✅ EOS 토큰\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# ✅ Alpaca-style 프롬프트 템플릿\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "# ✅ Step 1: instruction / output 필드 생성\n",
        "def to_alpaca_format(example):\n",
        "    return {\n",
        "        \"instruction\": example[\"question\"],\n",
        "        \"output\": example[\"answer\"]\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(to_alpaca_format)\n",
        "\n",
        "\n",
        "# ✅ Step 2: output 길이 필터링 (토큰 길이 516 이하)\n",
        "def is_output_short(example):\n",
        "    tokenized = tokenizer(example[\"output\"], truncation=False, add_special_tokens=False)\n",
        "    return len(tokenized[\"input_ids\"]) <= 516\n",
        "\n",
        "filtered_dataset = dataset.filter(is_output_short)\n",
        "\n",
        "# ✅ Step 3: text 필드 생성\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\": texts }\n",
        "\n",
        "formatted_dataset = filtered_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# ✅ 결과 확인\n",
        "print(f\"📊 필터링 후 샘플 수: {len(formatted_dataset)}\")\n",
        "print(formatted_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxocSwZYa8wH"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported  # BFloat16 지원 여부 확인 함수 임포트\n",
        "\n",
        "\n",
        "# SFTTrainer를 사용하여 모델 학습 설정\n",
        "# SFTTrainer 인스턴스 생성\n",
        "trainer = SFTTrainer(\n",
        "    model = model,                           # 학습할 모델\n",
        "    tokenizer = tokenizer,                   # 사용할 토크나이저\n",
        "    train_dataset = formatted_dataset,                 # 학습할 데이터셋 ★★★★★★★★\n",
        "    dataset_text_field = \"text\",             # 데이터셋의 텍스트 필드 이름 ★★★★★★★★\n",
        "    max_seq_length = 1024,         # 최대 시퀀스 길이\n",
        "    dataset_num_proc = 2,                    # 데이터셋 전처리에 사용할 프로세스 수 cpu\n",
        "    packing = False,                         # 짧은 시퀀스의 경우 packing을 비활성화 (학습 속도 5배 향상 가능)\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,     # 디바이스 당 배치 사이즈\n",
        "        gradient_accumulation_steps = 4,     # 그래디언트 누적 단계 수\n",
        "        warmup_steps = 5,                     # 워밍업 스텝 수\n",
        "        # num_train_epochs = 1,               # 전체 학습 에폭 수 설정 가능\n",
        "        max_steps = 60,                       # 최대 학습 스텝 수\n",
        "        learning_rate = 2e-4,                 # 학습률\n",
        "        fp16 = not is_bfloat16_supported(),   # BFloat16 지원 여부에 따라 FP16 사용\n",
        "        bf16 = is_bfloat16_supported(),       # BFloat16 사용 여부\n",
        "        logging_steps = 1,                    # 로깅 빈도\n",
        "        optim = \"adamw_8bit\",                  # 옵티마이저 설정 (8비트 AdamW)\n",
        "        weight_decay = 0.01,                  # 가중치 감쇠\n",
        "        lr_scheduler_type = \"linear\",         # 학습률 스케줄러 타입\n",
        "        seed = 3407,                           # 랜덤 시드 설정\n",
        "        output_dir = \"outputs\",                # 출력 디렉토리\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjhFftZTa8wI"
      },
      "source": [
        "학습 실행\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mELcfbeda8wI"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체적인 수렴 속도 및 안정성은 DoRA가 아주 약간 더 빠르고 부드럽게 감소하는 경향을 보입니다.\n",
        "\n",
        "60 step 기준 최종 Loss:\n",
        "\n",
        "- QLoRA: 1.7695\n",
        "- DoRA: 1.7664\n"
      ],
      "metadata": {
        "id": "YUIwdpEHi58r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTZEcDTSa8wI"
      },
      "outputs": [],
      "source": [
        "# 모델 저장 로컬폴더에다가 저장하는 방식\n",
        "model.save_pretrained(\"Dora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"Dora_model\")\n",
        "\n",
        "# 이 이외에 허깅페이스나 다른 hub에 push해서 저장하는 방법이 있음\n",
        "# 다만, 업로드 속도와 다운로드 속도를 고려해야함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjNPOOdNa8wI"
      },
      "source": [
        "### (4) 파인튜닝된 DoRA 모델 평가 (Inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'런타임 - 세션 다시 시작' 후 실행"
      ],
      "metadata": {
        "id": "WRFoab01V0CQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TwMfmMAa8wI"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 저장된 경로 지정\n",
        "save_directory = \"Dora_model\"\n",
        "\n",
        "# 모델과 토크나이저 불러오기\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_directory,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,  # 양자화 옵션을 동일하게 설정\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QWxiXGna8wI"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# ✅ Alpaca-style 프롬프트 템플릿\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# 추론해보기\n",
        "FastLanguageModel.for_inference(model)  # 네이티브 2배 빠른 추론 활성화\n",
        "\n",
        "# 추론을 위한 입력 준비\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"최저임금제도란 무엇이며, 최저임금액은 어떻게 결정되는지요?\", # 인스트럭션 (명령어)\n",
        "        \"\", # 출력 - 생성할 답변을 비워둠\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")  # 텐서를 PyTorch 형식으로 변환하고 GPU로 이동\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)  # 토크나이저를 사용하여 스트리머 초기화\n",
        "\n",
        "# 모델을 사용하여 텍스트 생성 및 스트리밍 출력\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DoRA는 이론적으로 더 정규화된 학습과 안정적인 수렴을 제공하지만,\n",
        "본 실습에서는 응답 생성 품질에서 QLoRA가 더 우수한 결과를 보였습니다."
      ],
      "metadata": {
        "id": "Aj67laI4jE_u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4AnKcW0gAC_"
      },
      "source": [
        "## 4) 외부 지식 기반 답변 생성 (RAG 적용)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 RAG란? (Retrieval-Augmented Generation)\n",
        "\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)**는 LLM이 응답을 생성할 때,  \n",
        "외부 지식(문서, DB 등)을 검색하여 함께 활용하는 방식입니다.\n",
        "\n",
        "\n",
        "기존 LLM은 학습된 범위 내의 지식만 활용할 수 있어,  \n",
        "최신 정보나 외부 데이터 기반 응답에는 한계가 있습니다.  \n",
        "이를 보완하기 위해 RAG 구조에서는 다음 두 단계가 추가됩니다:\n",
        "\n",
        "\n",
        "1. **Retrieval (검색)**  \n",
        "   - 사용자의 질문에 대해 외부 문서에서 관련 정보를 검색합니다.  \n",
        "   - 의미 기반 임베딩 검색(Dense Retrieval) 또는 키워드 기반 검색(BM25 등)을 활용할 수 있습니다.\n",
        "\n",
        "2. **Augmented Generation (응답 생성)**  \n",
        "   - 검색된 정보를 LLM의 입력 프롬프트에 함께 넣고 응답을 생성합니다.  \n",
        "   - 문서 내용을 반영한 더 정확하고 풍부한 응답을 유도할 수 있습니다.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "✅ RAG의 구성 요소\n",
        "\n",
        "| 구성 요소       | 설명 |\n",
        "|----------------|------|\n",
        "| Query Encoder   | 사용자의 질문을 임베딩하여 검색에 사용 |\n",
        "| Retriever       | FAISS 등으로 벡터 유사도 기반 문서 검색 |\n",
        "| Document Encoder| 문서들을 사전에 임베딩해 벡터 DB로 저장 |\n",
        "| LLM             | 검색된 정보를 기반으로 답변 생성 (ex. QLoRA 튜닝된 LLM) |\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "✅ RAG의 장점\n",
        "\n",
        "- 🧠 **최신 정보 반영 가능**: 정적 파인튜닝 모델의 한계를 극복  \n",
        "- 📚 **도메인 지식 강화**: 특정 분야 문서를 기반으로 더 정밀한 응답 생성  \n",
        "- 🛠 **파인튜닝 없이도 커스터마이징** 가능: 검색 문서만 바꿔도 챗봇 성능 개선 가능\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "✅ RAG가 필요한 상황 예시\n",
        "\n",
        "- \"2025년 최신 최저임금 기준은?\" → 파인튜닝 모델은 모름, 검색 기반 필요  \n",
        "- \"민법 제750조와 관련된 사례 알려줘\" → 외부 법률 문서와 연결 필요  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "x_3y3GNRQ_ta"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqz70DoKgyS8"
      },
      "source": [
        "### (1) Dense Retrieval (임베딩 기반 검색)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPMWIJ0zPRQx"
      },
      "source": [
        "Dense Retrieval은 사용자의 질문(Query)을 임베딩 벡터로 변환한 후, 문서(또는 대화 기록)들을 동일한 벡터 공간에 투영하여 가장 유사한 문서를 검색하는 방식입니다.\n",
        "\n",
        "기존의 키워드 기반 검색(BM25 등)과 달리, 의미 기반 유사도(MEANING-BASED SIMILARITY)를 파악할 수 있어 자연어 질문에 더 강력한 검색 성능을 보입니다.\n",
        "\n",
        "이 실습에서는 파인튜닝한 챗봇 모델과 함께 사용할 수 있도록, jihye-moon/LawQA-Ko 데이터셋의 질문-답변 문장을 벡터화하고, 사용자 질문과 가장 유사한 기존 법률 문서를 검색하여 답변 생성에 활용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19eWl9O8PYhS"
      },
      "source": [
        "구현 흐름\n",
        "\n",
        "- 문서 구성: jihye-moon/LawQA-Ko 데이터셋의 질문 + 답변을 하나의 문서로 구성\n",
        "\n",
        "- 문서 임베딩: sentence-transformers 기반 KoSBERT 또는 E5 모델로 임베딩 수행\n",
        "\n",
        "- 인덱스 생성: FAISS를 이용한 벡터 DB 구축\n",
        "\n",
        "- 검색: 사용자 질문 → 임베딩 → 유사도 기반 가장 유사한 문서 검색\n",
        "\n",
        "- 출력: 검색된 문서를 기반으로 LLM 프롬프트에 삽입하여 응답 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6BMSUeRQFeu"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 1. 문서 준비 : 질문 + 답변 하나의 문장으로 구성\n",
        "\n",
        "- 검색 문서는 단일 문장 형태로 구성합니다.\n",
        "- \"질문\\n답변\" 형태로 결합하여 임베딩 벡터로 변환합니다"
      ],
      "metadata": {
        "id": "dlAnUFpXR0R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'런타임 - 세션 다시 시작' 후 실행"
      ],
      "metadata": {
        "id": "FFBIzDE1lVtf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vT6oqlEJIKm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset(\"jihye-moon/LawQA-Ko\", split=\"train\")\n",
        "\n",
        "# 문서 리스트 생성\n",
        "documents = []\n",
        "\n",
        "for example in dataset.select(range(1000)):  # 예시로 1000개만 사용\n",
        "    question = example[\"question\"].strip()\n",
        "    answer = example[\"answer\"].strip()\n",
        "    full_text = f\"{question}\\n{answer}\"  # 질문과 답변 결합\n",
        "    documents.append(full_text)\n",
        "\n",
        "print(f\"✅ 문서 수: {len(documents)}\")\n",
        "print(\"📄 첫 번째 문서:\\n\", documents[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. 문서 임베딩: KoSBERT 기반\n",
        "- sentence-transformers의 jhgan/ko-sbert-nli 모델을 사용합니다.\n",
        "- 모든 문서를 벡터화하여 numpy 배열로 저장합니다.\n",
        "\n",
        "\n",
        "```\n",
        "KoSBERT란?\n",
        "KoSBERT는 문장을 벡터로 변환하는 한국어 특화 임베딩 모델입니다.\n",
        "\n",
        "- SBERT(Sentence-BERT)의 구조를 기반으로 하며,\n",
        "- 한국어 자연어 추론(NLI) 데이터로 파인튜닝 되어 있어 문장 간 의미 유사도 계산에 탁월합니다.\n",
        "- 본 실습에서는 sentence-transformers 라이브러리를 통해 jhgan/ko-sbert-nli 모델을 사용합니다.\n",
        "- 이를 통해 \"질문과 의미적으로 가장 비슷한 문서\"를 빠르게 찾을 수 있습니다.\n",
        "```"
      ],
      "metadata": {
        "id": "whAXiVaUR6VF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiX_xZngQb0n"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# KoSBERT 모델 로드\n",
        "embedder = SentenceTransformer(\"jhgan/ko-sbert-nli\")\n",
        "\n",
        "# 문서 임베딩\n",
        "doc_embeddings = embedder.encode(documents, convert_to_numpy=True, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 3. FAISS를 이용한 벡터 인덱스 생성\n",
        "- FAISS는 고속 벡터 검색 라이브러리로, 문서 간 유사도 기반 검색에 활용됩니다."
      ],
      "metadata": {
        "id": "6kd5R6f7R9nB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmNDIaIBQddM"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# 임베딩 차원 확인\n",
        "dimension = doc_embeddings.shape[1]\n",
        "\n",
        "# 벡터 인덱스 생성\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "print(\"✅ FAISS 인덱스 생성 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. 유사 문서 검색 함수 정의"
      ],
      "metadata": {
        "id": "u4IkVpP9SBCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_documents(query, top_k=3):\n",
        "    # 쿼리 임베딩\n",
        "    query_vec = embedder.encode([query])\n",
        "\n",
        "    # 유사도 기반 검색\n",
        "    D, I = index.search(np.array(query_vec), k=top_k)\n",
        "\n",
        "    # 상위 문서 반환\n",
        "    return [documents[i] for i in I[0]]"
      ],
      "metadata": {
        "id": "XFhgro69SBfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시: 질문을 입력해 관련 문서 검색\n",
        "query = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "results = search_documents(query, top_k=3)\n",
        "\n",
        "print(\"🧠 사용자 질문:\", query)\n",
        "print(\"\\n📄 검색된 유사 문서:\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n🔹 [Top {i+1}]\\n{doc[:300]}...\")"
      ],
      "metadata": {
        "id": "fMAa0YeRSD7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5. 검색 결과를 기반으로 LLM에 질문 프롬프트 구성\n",
        "\n",
        "Dense Retrieval로 가져온 문서들을 LLM 입력 프롬프트에 삽입합니다.\n",
        "우리는 Alpaca-style 모델을 사용하고 있으므로, 프롬프트는 아래와 같은 형태로 구성합니다:\n",
        "\n",
        "```\n",
        "다음은 참고할 수 있는 문서들입니다:\n",
        "{retrieved_doc_1}\n",
        "\n",
        "---\n",
        "\n",
        "{retrieved_doc_2}\n",
        "\n",
        "---\n",
        "\n",
        "...\n",
        "\n",
        "### Instruction:\n",
        "{user_query}\n",
        "\n",
        "### Response:\n",
        "```"
      ],
      "metadata": {
        "id": "VzpUO9hBSQZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "# ✅ 모델 로드\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", # finetuning 이전 원본 모델로 실습\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True\n",
        ")\n",
        "\n",
        "# ✅ 추론 최적화\n",
        "FastLanguageModel.for_inference(model)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "ns8eq2VtSbDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "원본에 바로 질문 했을 때 결과"
      ],
      "metadata": {
        "id": "9b5c7SP4abox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "\n",
        "# ✅ 입력을 모델이 처리할 수 있는 텐서로 변환\n",
        "inputs = tokenizer(user_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(f\"📝 질문: {user_input}\")\n",
        "\n",
        "# ✅ 모델 응답 생성 (추론 옵션 적용)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,        # 응답 최대 길이 제한\n",
        "    temperature=0.5,           # 온도 조절을 통해 일관된 응답 유도\n",
        "    top_p=0.85,                # 상위 확률 토큰만 샘플링\n",
        "    repetition_penalty=1.2,    # 반복 방지\n",
        "    do_sample=True             # 샘플링 방식 적용\n",
        ")\n",
        "\n",
        "# ✅ 응답 디코딩\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"🤖 응답:\", decoded_output)"
      ],
      "metadata": {
        "id": "QNNdEez3af_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6. LLM 프롬프트 구성 및 응답 생성"
      ],
      "metadata": {
        "id": "N0EtCiofSc5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_with_retrieval(query, top_k=3, max_context_len=1500):\n",
        "    # 🔍 문서 검색\n",
        "    retrieved_docs = search_documents(query, top_k=top_k)\n",
        "\n",
        "    # 📄 문서 병합 (너무 길면 자름)\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
        "    if len(context) > max_context_len:\n",
        "        context = context[:max_context_len]\n",
        "\n",
        "    # 🧾 프롬프트 구성 (Alpaca-style)\n",
        "    prompt = (\n",
        "        f\"다음은 참고할 수 있는 문서들입니다:\\n{context}\\n\\n\"\n",
        "        f\"### Instruction:\\n{query}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "    )\n",
        "\n",
        "    # 🔢 토크나이즈\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # 🤖 응답 생성\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    # 🧾 디코딩\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"### Response:\" in decoded:\n",
        "        response = decoded.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        response = decoded.strip()\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "-TlDmDUHSea4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 실행 예시\n",
        "\n",
        "query = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "response = generate_answer_with_retrieval(query)\n",
        "\n",
        "print(\"🧠 사용자 질문:\", query)\n",
        "print(\"\\n🤖 LLM 응답:\\n\", response)"
      ],
      "metadata": {
        "id": "FMh-GuwDSgzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jb1US99g1fM"
      },
      "source": [
        "### (2) Hybrid Search (BM25 + 벡터 검색)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFBRxA8SIvd"
      },
      "source": [
        "Hybrid Search는 키워드 기반의 BM25 검색과 의미 기반의 **임베딩 검색(Dense Retrieval)**을 결합하여 정확도와 다양성을 모두 확보할 수 있는 검색 기법입니다.\n",
        "\n",
        "- BM25는 텍스트 기반 키워드 매칭에 강하고,\n",
        "- Dense Retrieval은 질문 의미와 유사한 문서를 찾아내는 데 효과적입니다.\n",
        "- 두 검색 결과를 통합하여 평균 점수를 기반으로 최종 순위를 산출합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "BM25란?\n",
        "BM25(Best Matching 25)는 정보 검색(IR)에서 가장 널리 쓰이는 키워드 기반 문서 순위화 알고리즘입니다.\n",
        "검색어와 문서 사이의 단어 빈도(TF)와 역문서 빈도(IDF)를 바탕으로, 관련도 점수를 계산합니다.\n",
        "\n",
        "- 단순한 단어 매칭이지만, 질의어와 문서 간의 텍스트적 일치를 잘 반영합니다.\n",
        "- TF-IDF의 확장 형태로, 문서 길이 보정 기능이 포함되어 있습니다.\n",
        "- 본 실습에서는 TfidfVectorizer + cosine_similarity로 간단하게 BM25 유사도 대체 구현을 사용합니다.\n",
        "```"
      ],
      "metadata": {
        "id": "-5u1JFwXUb5R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SULqt4d_SO0c"
      },
      "source": [
        "BM25는 일반적으로 Whoosh, Elasticsearch, Lucene, scikit-learn TfidfVectorizer 등을 사용합니다.\n",
        "Colab에서는 간편하게 TfidfVectorizer를 활용한 BM25 유사도 기반 접근을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "구현 흐름\n",
        "\n",
        "- 문서 준비 (앞 단계 동일)\n",
        "\n",
        "- 임베딩 벡터 & TF-IDF 벡터 준비\n",
        "\n",
        "- Hybrid 검색 함수 정의\n",
        "\n",
        "- LLM 프롬프트 구성 및 응답 생성\n",
        "\n",
        "-  Hybrid 검색 기반 응답 함수 정의"
      ],
      "metadata": {
        "id": "jkSJp-z9Ta7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. 문서 준비 (앞 단계 동일)"
      ],
      "metadata": {
        "id": "-yaVGbTNTloD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCY1ebNeSNrR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"jihye-moon/LawQA-Ko\", split=\"train\")\n",
        "\n",
        "documents = []\n",
        "for example in dataset.select(range(1000)):\n",
        "    question = example[\"question\"].strip()\n",
        "    answer = example[\"answer\"].strip()\n",
        "    full_text = f\"{question}\\n{answer}\"\n",
        "    documents.append(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. 임베딩 벡터 & TF-IDF 벡터 준비"
      ],
      "metadata": {
        "id": "0phD0E-eTpnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# KoSBERT 임베딩\n",
        "embedder = SentenceTransformer(\"jhgan/ko-sbert-nli\")\n",
        "doc_embeddings = embedder.encode(documents, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# FAISS 인덱스 생성\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)"
      ],
      "metadata": {
        "id": "dTx3OS-2ToXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. Hybrid 검색 함수 정의"
      ],
      "metadata": {
        "id": "FDdiYSEwTtDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_search(query, top_k=3):\n",
        "    # 1. TF-IDF (BM25 유사도)\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    bm25_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "    # 2. Dense 임베딩 유사도\n",
        "    query_dense = embedder.encode([query])\n",
        "    D, I_dense = index.search(np.array(query_dense), k=top_k * 2)  # 후보 넉넉히\n",
        "\n",
        "    # 3. Hybrid 점수 계산\n",
        "    hybrid_scores = {}\n",
        "    dense_indices = I_dense[0]\n",
        "    bm25_top = np.argsort(bm25_scores)[-top_k*2:]\n",
        "\n",
        "    for i in set(dense_indices).union(set(bm25_top)):\n",
        "        bm25_score = bm25_scores[i]\n",
        "        dense_score = 1 - np.linalg.norm(query_dense - doc_embeddings[i])\n",
        "        hybrid_scores[i] = (bm25_score + dense_score) / 2\n",
        "\n",
        "    # 4. 상위 문서 반환\n",
        "    sorted_docs = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    return [documents[i] for i, _ in sorted_docs]"
      ],
      "metadata": {
        "id": "kkvIdGwyTvIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. LLM 프롬프트 구성 및 응답 생성"
      ],
      "metadata": {
        "id": "3ac9BznlTwj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from unsloth import FastLanguageModel\n",
        "# from transformers import TextStreamer\n",
        "# import torch\n",
        "\n",
        "# # ✅ 모델 로드\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\", # finetuning 이전 원본 모델로 실습\n",
        "#     max_seq_length = 2048,\n",
        "#     dtype = None,\n",
        "#     load_in_4bit = True\n",
        "# )\n",
        "\n",
        "# # ✅ 추론 최적화\n",
        "# FastLanguageModel.for_inference(model)\n",
        "# tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "MXAZRTEcTxFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "\n",
        "# ✅ 입력을 모델이 처리할 수 있는 텐서로 변환\n",
        "inputs = tokenizer(user_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(f\"📝 질문: {user_input}\")\n",
        "\n",
        "# ✅ 모델 응답 생성 (추론 옵션 적용)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=256,        # 응답 최대 길이 제한\n",
        "    temperature=0.5,           # 온도 조절을 통해 일관된 응답 유도\n",
        "    top_p=0.85,                # 상위 확률 토큰만 샘플링\n",
        "    repetition_penalty=1.2,    # 반복 방지\n",
        "    do_sample=True             # 샘플링 방식 적용\n",
        ")\n",
        "\n",
        "# ✅ 응답 디코딩\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"🤖 응답:\", decoded_output)"
      ],
      "metadata": {
        "id": "mpYaS5OcbmTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5. Hybrid 검색 기반 응답 함수 정의"
      ],
      "metadata": {
        "id": "Ujx8lS3DTyIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hybrid_answer(query, top_k=3, max_context_len=1500):\n",
        "    retrieved_docs = hybrid_search(query, top_k=top_k)\n",
        "    context = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
        "    if len(context) > max_context_len:\n",
        "        context = context[:max_context_len]\n",
        "\n",
        "    prompt = (\n",
        "        f\"다음은 참고할 수 있는 문서들입니다:\\n{context}\\n\\n\"\n",
        "        f\"### Instruction:\\n{query}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"### Response:\" in decoded:\n",
        "        response = decoded.split(\"### Response:\")[-1].strip()\n",
        "    else:\n",
        "        response = decoded.strip()\n",
        "    return response"
      ],
      "metadata": {
        "id": "KivOK2PxTzxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "response = generate_hybrid_answer(query)\n",
        "\n",
        "print(\"🧠 사용자 질문:\", query)\n",
        "print(\"\\n🤖 LLM 응답:\\n\", response)"
      ],
      "metadata": {
        "id": "FEZAd7WUT4hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqKwQE_Fg3m-"
      },
      "source": [
        "### (3) Reranking을 통한 검색 결과 개선"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpVcmKvWV4HW"
      },
      "source": [
        "Reranking은 Dense Search 또는 Hybrid Search로 먼저 후보 문서를 추출한 후,\n",
        "질문과 문서 간의 의미적 일치도를 더 정밀하게 재측정하여 최종 순위를 다시 정렬하는 기법입니다.\n",
        "\n",
        "- 초기 검색 단계에서는 빠르게 top-k 후보 문서를 추출\n",
        "- 그 다음, CrossEncoder 같은 문서-질문 쌍 비교 모델을 사용해 정확한 순위 재조정\n",
        "- LLM에는 최종 상위 문서만 전달하여 더 정확하고 정보성 높은 응답 생성 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "주요 용어 정리\n",
        "- Cross-Encoder :\n",
        "문서-질문 쌍을 하나의 입력으로 받아 의미 유사도를 정밀하게 예측하는 모델\n",
        "(ex. \"질문 [SEP] 문서\" 형태로 입력 → relevance score 출력)\n",
        "\n",
        "- MS-MARCO :\n",
        "Microsoft가 만든 대규모 질의응답 데이터셋으로, 검색 및 rerank 학습에 많이 사용됨.\n",
        "\n",
        "- KoBERT 기반 CrossEncoder :\n",
        "한국어 검색 정제에 적합한 형태로 사전학습된 문장 쌍 비교 모델 (MS-MARCO 기반 KoSimCSE 등)"
      ],
      "metadata": {
        "id": "BK1Rb9WEb0gp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cASBkMDLWA9Y"
      },
      "source": [
        "구현 흐름\n",
        "- Hybrid Search 기반 초기 후보 문서 10개 추출\n",
        "- Reranker 모델로 문서 순위 재조정\n",
        "-  LLM 프롬프트 구성 및 응답 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. Hybrid Search 기반 초기 후보 문서 10개 추출"
      ],
      "metadata": {
        "id": "YukSTBjob93w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_yUhRHsWHry"
      },
      "outputs": [],
      "source": [
        "# 사용자 질문\n",
        "query = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "\n",
        "# 1. TF-IDF 유사도 계산\n",
        "query_tfidf = vectorizer.transform([query])\n",
        "bm25_scores = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "bm25_top = np.argsort(bm25_scores)[::-1][:10]\n",
        "\n",
        "# 2. Dense 임베딩 유사도 계산\n",
        "query_dense = embedder.encode([query])\n",
        "D, I_dense = index.search(np.array(query_dense), k=10)\n",
        "dense_top = I_dense[0]\n",
        "\n",
        "# 3. Hybrid Score 계산 (BM25 + Dense 평균)\n",
        "hybrid_scores = {}\n",
        "for i in set(bm25_top).union(set(dense_top)):\n",
        "    bm25_score = bm25_scores[i]\n",
        "    dense_score = 1 - np.linalg.norm(query_dense - doc_embeddings[i])\n",
        "    hybrid_scores[i] = (bm25_score + dense_score) / 2\n",
        "\n",
        "# 후보 문서 추출 (10개)\n",
        "top_candidates = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "candidate_docs = [(query, documents[idx]) for idx, _ in top_candidates]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. Reranker 모델로 문서 순위 재조정"
      ],
      "metadata": {
        "id": "ktxMkutScOOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50_4JhnWWMbN"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# CrossEncoder 로드 (MS-MARCO 학습 기반)\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "# 문서 쌍 → 유사도 점수 계산\n",
        "rerank_scores = reranker.predict(candidate_docs)\n",
        "\n",
        "# 유사도 기반 재정렬\n",
        "reranked = sorted(zip(candidate_docs, rerank_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# 최종 top-k 문서 선택 (예: 3개)\n",
        "top_k = 3\n",
        "reranked_docs = [doc for (query, doc), score in reranked[:top_k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. LLM 프롬프트 구성 및 응답 생성"
      ],
      "metadata": {
        "id": "uxA9AuB-cSv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9sGBFZlWQOA"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# 프롬프트용 문서 병합\n",
        "context = \"\\n\\n---\\n\\n\".join(reranked_docs)\n",
        "\n",
        "# LLM 프롬프트 구성 (Alpaca-style)\n",
        "prompt = (\n",
        "    f\"다음은 참고할 수 있는 문서들입니다:\\n{context}\\n\\n\"\n",
        "    f\"### Instruction:\\n{query}\\n\\n\"\n",
        "    f\"### Response:\\n\"\n",
        ")\n",
        "\n",
        "# 토크나이즈\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# 응답 생성\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "# 응답 디코딩\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "response = decoded.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# 출력\n",
        "print(\"🧠 사용자 질문:\", query)\n",
        "print(\"\\n🤖 LLM 응답:\\n\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cibU5EUg6-E"
      },
      "source": [
        "### (4) Auto-merging Retrieval (자동 청킹 + 병합 전략)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pdjlwDXXugw"
      },
      "source": [
        "Auto-merging Retrieval은 검색된 여러 문서가 너무 짧거나 내용이 흩어져 있을 경우, 이들을 하나로 병합하여 LLM이 더 잘 이해할 수 있도록 돕는 RAG 전략입니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "이 전략은 다음과 같은 상황에서 유용합니다:\n",
        "- 개별 문서의 길이가 짧아 정보가 부족할 때\n",
        "- 여러 문서가 서로 연관된 정보를 가지고 있을 때\n",
        "- LLM의 context window를 고려하여 효율적인 정보 입력이 필요할 때"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMWxb5LKXzUb"
      },
      "source": [
        "구현 흐름\n",
        "1. Hybrid Retrieval 또는 Dense Retrieval을 통해 Top-K 문서를 검색\n",
        "2. 각 문서를 적절히 청크 또는 연결하여 하나의 context로 병합\n",
        "3. 병합된 문서를 프롬프트에 삽입하여 LLM 응답 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. 사용자 질문 입력 및 문서 검색"
      ],
      "metadata": {
        "id": "qhr6vftlc76N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaY3BDoeX0EO"
      },
      "outputs": [],
      "source": [
        "query = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "\n",
        "# 기존 hybrid_search 또는 search_documents 사용 가능\n",
        "documents_to_merge = hybrid_search(query, top_k=5)  # 또는 search_documents(query, top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. 문서 병합 (자동 청킹)"
      ],
      "metadata": {
        "id": "yvs6NPg-dCtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgi8RkvlX-DL"
      },
      "outputs": [],
      "source": [
        "# 병합된 context 구성\n",
        "merged_context = \"\\n\\n---\\n\\n\".join(documents_to_merge)\n",
        "\n",
        "# LLM 입력 길이 고려하여 context 길이 제한 (예: 3500자)\n",
        "max_char_len = 3500\n",
        "if len(merged_context) > max_char_len:\n",
        "    merged_context = merged_context[:max_char_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. 프롬프트 구성 및 응답 생성"
      ],
      "metadata": {
        "id": "7qv9qftLdGfI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvfTm6nCYAie"
      },
      "outputs": [],
      "source": [
        "# Alpaca-style 프롬프트 구성\n",
        "prompt = (\n",
        "    f\"다음은 참고할 수 있는 문서들입니다:\\n{merged_context}\\n\\n\"\n",
        "    f\"### Instruction:\\n{query}\\n\\n\"\n",
        "    f\"### Response:\\n\"\n",
        ")\n",
        "\n",
        "# 토크나이즈 및 LLM 추론\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "# 응답 디코딩\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "response = decoded.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# 출력\n",
        "print(\"🧠 사용자 질문:\", query)\n",
        "print(\"\\n📄 병합된 문서 (context 일부):\\n\", merged_context[:500], \"...\")\n",
        "print(\"\\n🤖 LLM 응답:\\n\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 참고: 청킹 전략이 중요한 이유\n",
        "- LLM은 제한된 context window 내에서만 정보를 처리할 수 있으므로, 너무 많은 문서를 그대로 넣으면 잘리는 정보가 발생할 수 있습니다.\n",
        "- 이럴 때 관련된 문서끼리 병합하여 하나의 흐름 있는 context를 제공하면 응답 품질이 향상됩니다."
      ],
      "metadata": {
        "id": "l0T5EFPbdSzV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTj6aUIVg9_S"
      },
      "source": [
        "### (5) Self-Query를 활용한 메타데이터 필터링\n",
        "\n",
        "\n",
        "Self-Query 기반 메타데이터 필터링은 사용자의 질문에서 **의미 있는 키워드나 주제(메타데이터)**를 추출하고,\n",
        "이 키워드를 기반으로 검색 대상 문서 자체를 선별하는 방법입니다.\n",
        "\n",
        "\n",
        "즉, \"검색 전에 관련 있는 문서만 골라서 검색\"하는 전략입니다.\n",
        "\n",
        "\n",
        "- 장점\n",
        "    - 검색 정확도 향상\n",
        "    - LLM 프롬프트 축소\n",
        "    - 응답 품질 향상"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Q7i01JZMF-"
      },
      "source": [
        "구현 흐름\n",
        "1. 사용자 질문에서 키워드 추출\n",
        "2. 메타데이터 조건에 맞는 문서 필터링\n",
        "3. Dense 임베딩 검색 수행\n",
        "4. LLM 프롬프트 구성 및 응답 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. 사용자 질문에서 키워드 추출\n",
        "\n",
        "\n",
        "간단한 키워드 리스트 기반 추출을 사용하지만, 실제 서비스에서는\n",
        "NER, Keyphrase Extraction, BERT 기반 분류기로 고도화할 수 있습니다."
      ],
      "metadata": {
        "id": "7Yav0bYGeWcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9QAKYfgZZSi"
      },
      "outputs": [],
      "source": [
        "# ✅ 사용자 질문\n",
        "query = \"부동산 계약 해지 조건은 어떻게 되나요?\"\n",
        "\n",
        "# ✅ 주요 키워드 리스트 (주제별 법률 용어 정의)\n",
        "keyword_list = [\"부동산\", \"계약 해지\"]\n",
        "\n",
        "# ✅ 질문에 포함된 키워드 추출\n",
        "keywords = [kw for kw in keyword_list if kw in query]\n",
        "\n",
        "print(\"🧠 추출된 키워드:\", keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. 메타데이터 조건에 맞는 문서 필터링\n",
        "\n",
        "이전 단계에서 구성한 documents 리스트 중에서,\n",
        "질문 키워드를 모두 포함한 문서만 추출합니다."
      ],
      "metadata": {
        "id": "A7KvEJoQeawO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXSkTfYbZgf8"
      },
      "outputs": [],
      "source": [
        "filtered_documents = []\n",
        "filtered_embeddings = []\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    if all(keyword in doc for keyword in keywords):  # 모든 키워드 포함 시 선택\n",
        "        filtered_documents.append(doc)\n",
        "        filtered_embeddings.append(doc_embeddings[i])\n",
        "\n",
        "print(f\"✅ 필터링된 문서 수: {len(filtered_documents)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. Dense 임베딩 검색 수행"
      ],
      "metadata": {
        "id": "5Fzbyoc_esSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h1y8BuoZkFu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ✅ 사용자 질문 임베딩\n",
        "query_vec = embedder.encode([query])\n",
        "filtered_embeddings = np.array(filtered_embeddings)\n",
        "\n",
        "# ✅ 유사도 기반 검색 (Cosine 유사도)\n",
        "similarities = np.dot(filtered_embeddings, query_vec.T).squeeze()\n",
        "top_k = 3\n",
        "top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "# ✅ 최종 검색 문서 선택\n",
        "top_docs = [filtered_documents[i] for i in top_indices]\n",
        "\n",
        "top_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. LLM 프롬프트 구성 및 응답 생성"
      ],
      "metadata": {
        "id": "4m_m9QlHe25X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 문서 병합 및 context 구성\n",
        "merged_context = \"\\n\\n---\\n\\n\".join(top_docs)\n",
        "max_char_len = 1800\n",
        "if len(merged_context) > max_char_len:\n",
        "    merged_context = merged_context[:max_char_len]\n",
        "\n",
        "# ✅ Alpaca 스타일 프롬프트 생성\n",
        "prompt = (\n",
        "    f\"다음은 참고할 수 있는 문서들입니다:\\n{merged_context}\\n\\n\"\n",
        "    f\"### Instruction:\\n{query}\\n\\n\"\n",
        "    f\"### Response:\\n\"\n",
        ")\n",
        "\n",
        "# ✅ LLM 추론\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "# ✅ 응답 디코딩\n",
        "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "if \"### Response:\" in decoded:\n",
        "    response = decoded.split(\"### Response:\")[-1].strip()\n",
        "else:\n",
        "    response = decoded.strip()\n",
        "\n",
        "print(\"🧠 사용자 질문:\", query)\n",
        "print(\"\\n🤖 LLM 응답:\\n\", response)"
      ],
      "metadata": {
        "id": "m3ikrtZoe2CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N9MIKCDgCy9"
      },
      "source": [
        "## 5) 한국어 챗봇 데모 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어 법률 Q&A 챗봇을 실제로 구동할 수 있는 데모 인터페이스를 구축합니다.\n",
        "\n",
        "사용자는 질문을 입력하고, 챗봇은 법률 데이터를 기반으로 검색(RAG)한 뒤 응답을 생성합니다."
      ],
      "metadata": {
        "id": "AbXyArUofVcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 챗봇 데모 구성 요소\n",
        "\n",
        "- Streamlit 또는 Colab Form 기반 인터페이스\n",
        "- 입력창을 통해 사용자의 질문 수집\n",
        "- RAG 적용 여부를 선택할 수 있도록 구성\n",
        "- 선택된 검색 방식 (Dense / Hybrid / Rerank / Metadata 기반)으로 문서 검색\n",
        "- 검색된 문서들을 기반으로 LLM 응답 생성 후 출력"
      ],
      "metadata": {
        "id": "ttYelCB3fZX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. 모델 불러오기"
      ],
      "metadata": {
        "id": "SeZ1FOhWf6u8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gKAMTCT3WSJ"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 모델 불러오기 (파인튜닝 모델 또는 원본)\n",
        "model_path = \"Dora_model\"  # or \"Dora_model\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_path,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. RAG 응답 생성 함수 정의"
      ],
      "metadata": {
        "id": "vxJm_7W-gCAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_with_context(query, context_docs, max_context_len=1500):\n",
        "    context = \"\\n\\n---\\n\\n\".join(context_docs)\n",
        "    if len(context) > max_context_len:\n",
        "        context = context[:max_context_len]\n",
        "\n",
        "    prompt = (\n",
        "        f\"다음은 참고할 수 있는 문서들입니다:\\n{context}\\n\\n\"\n",
        "        f\"### Instruction:\\n{query}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "    )\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.split(\"### Response:\")[-1].strip()"
      ],
      "metadata": {
        "id": "aCvVUhUFf-Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. RAG 방식별 래핑 함수 구성"
      ],
      "metadata": {
        "id": "Vp_KqItCgEt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dense_answer(query):\n",
        "    top_docs = search_documents(query, top_k=3)\n",
        "    return generate_answer_with_context(query, top_docs)\n",
        "\n",
        "def generate_hybrid_answer(query):\n",
        "    top_docs = hybrid_search(query, top_k=3)\n",
        "    return generate_answer_with_context(query, top_docs)"
      ],
      "metadata": {
        "id": "2T3Aual0gDdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. 최종 UI 연결"
      ],
      "metadata": {
        "id": "1btJdUBIgQp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# 📌 입력 위젯 구성\n",
        "question_input = widgets.Textarea(\n",
        "    value=\"근로계약서 미작성 시 불이익은 어떤가요?\",\n",
        "    placeholder='질문을 입력하세요',\n",
        "    description='질문:',\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
        ")\n",
        "\n",
        "method_dropdown = widgets.Dropdown(\n",
        "    options=[\"Dense Retrieval\", \"Hybrid Retrieval\"],\n",
        "    value=\"Dense Retrieval\",\n",
        "    description=\"RAG 방식:\"\n",
        ")\n",
        "\n",
        "ask_button = widgets.Button(description=\"🤖 질문하기\", button_style='success')\n",
        "exit_button = widgets.Button(description=\"🛑 대화 종료\", button_style='danger')\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "# 📌 버튼 이벤트 핸들러\n",
        "def on_ask_clicked(b):\n",
        "    output.clear_output()\n",
        "    query = question_input.value.strip()\n",
        "    method = method_dropdown.value\n",
        "\n",
        "    if not query:\n",
        "        with output:\n",
        "            print(\"❗ 질문을 입력해주세요.\")\n",
        "        return\n",
        "\n",
        "    with output:\n",
        "        print(f\"📝 질문: {query}\\n\")\n",
        "        if method == \"Dense Retrieval\":\n",
        "            response = generate_dense_answer(query)\n",
        "        elif method == \"Hybrid Retrieval\":\n",
        "            response = generate_hybrid_answer(query)\n",
        "        else:\n",
        "            response = \"지원하지 않는 방식입니다.\"\n",
        "\n",
        "        print(f\"🤖 챗봇 응답:\\n{response}\")\n",
        "\n",
        "def on_exit_clicked(b):\n",
        "    output.clear_output()\n",
        "    with output:\n",
        "        print(\"👋 대화를 종료합니다. 감사합니다!\")\n",
        "\n",
        "# 📌 버튼 클릭 연결\n",
        "ask_button.on_click(on_ask_clicked)\n",
        "exit_button.on_click(on_exit_clicked)\n",
        "\n",
        "# 📌 UI 표시\n",
        "ui = widgets.VBox([\n",
        "    question_input,\n",
        "    method_dropdown,\n",
        "    widgets.HBox([ask_button, exit_button]),\n",
        "    output\n",
        "])\n",
        "\n",
        "display(ui)"
      ],
      "metadata": {
        "id": "brPodjhugQLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNixyCvTiG4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1fbfbebb192c480d96f5b80e2c301e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ba7925fd80f4173b82a0583283a159d",
              "IPY_MODEL_6f93357cb75a4780b3ddad52c4556551",
              "IPY_MODEL_35d51be188304889a515886b5aac20f1"
            ],
            "layout": "IPY_MODEL_e278bce3159d4201a1e38a112dfd55f5"
          }
        },
        "5ba7925fd80f4173b82a0583283a159d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad9806d38a041c185c9ff9ac15cbda8",
            "placeholder": "​",
            "style": "IPY_MODEL_39e3aa76b0d8403b9de1e8d4b968cfdc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6f93357cb75a4780b3ddad52c4556551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_404f6656a9354c76861b6e97289bc4c5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f9fac0640b3427b99c790ee94840b05",
            "value": 4
          }
        },
        "35d51be188304889a515886b5aac20f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0383d0fad41848ddbd52b9a1c43d043b",
            "placeholder": "​",
            "style": "IPY_MODEL_1c3ff582e28443d5b98d52367f520cdd",
            "value": " 4/4 [01:24&lt;00:00, 18.34s/it]"
          }
        },
        "e278bce3159d4201a1e38a112dfd55f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad9806d38a041c185c9ff9ac15cbda8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e3aa76b0d8403b9de1e8d4b968cfdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "404f6656a9354c76861b6e97289bc4c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f9fac0640b3427b99c790ee94840b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0383d0fad41848ddbd52b9a1c43d043b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c3ff582e28443d5b98d52367f520cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0db5dbe4a5642b286ac8e6dfe1f9ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68e6c02e1ec043e3b1dee1ba46ebf812",
              "IPY_MODEL_f1e90821f3af4129bc6b2ec21e109faa",
              "IPY_MODEL_c4182993cae747b5b8faa3cabe406016"
            ],
            "layout": "IPY_MODEL_155ba77be1a449c6afb12ebfc0309442"
          }
        },
        "68e6c02e1ec043e3b1dee1ba46ebf812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_600d691a3f4e494bb59e4fb02690c14d",
            "placeholder": "​",
            "style": "IPY_MODEL_21b6d8f66fe5426f9a5e21f2edf3900f",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "f1e90821f3af4129bc6b2ec21e109faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d2a925ac30e4c9d888c81b20f7a805c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9c553dea86d4c319e195d90e7c50562",
            "value": 0
          }
        },
        "c4182993cae747b5b8faa3cabe406016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8febbb44d6ea44f8804019a00881fe2e",
            "placeholder": "​",
            "style": "IPY_MODEL_d4985b03da554fd3ac58e0aa642d0cc2",
            "value": " 0/4 [00:27&lt;?, ?it/s]"
          }
        },
        "155ba77be1a449c6afb12ebfc0309442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "600d691a3f4e494bb59e4fb02690c14d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b6d8f66fe5426f9a5e21f2edf3900f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d2a925ac30e4c9d888c81b20f7a805c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9c553dea86d4c319e195d90e7c50562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8febbb44d6ea44f8804019a00881fe2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4985b03da554fd3ac58e0aa642d0cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e0ea9088abc4e5eb378503e294afe01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fa6847ca1714a7b81be18977d827ab3",
              "IPY_MODEL_f0cb596680b445f69bdc59d0978fab1a",
              "IPY_MODEL_67a5942c0247481fa8c907f6e703a1de"
            ],
            "layout": "IPY_MODEL_6e47ea66d19b4b72b1ef3b3d067af00d"
          }
        },
        "0fa6847ca1714a7b81be18977d827ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3747d95221334327bad3300eaa530e4f",
            "placeholder": "​",
            "style": "IPY_MODEL_b80db5254f974815b051ff95487e6001",
            "value": "README.md: 100%"
          }
        },
        "f0cb596680b445f69bdc59d0978fab1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171761887dd8496ead885e6f8db5ad77",
            "max": 1320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a8b794f078e4fbca6f24c2c8be73d4f",
            "value": 1320
          }
        },
        "67a5942c0247481fa8c907f6e703a1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836cc33d9a584660a4ebb64dfbedc44f",
            "placeholder": "​",
            "style": "IPY_MODEL_4eda05ba1e754cda92b5b9215ec0dbb8",
            "value": " 1.32k/1.32k [00:00&lt;00:00, 85.3kB/s]"
          }
        },
        "6e47ea66d19b4b72b1ef3b3d067af00d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3747d95221334327bad3300eaa530e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b80db5254f974815b051ff95487e6001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "171761887dd8496ead885e6f8db5ad77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8b794f078e4fbca6f24c2c8be73d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "836cc33d9a584660a4ebb64dfbedc44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eda05ba1e754cda92b5b9215ec0dbb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7de99cddaf0a4de79d45b7629522424e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d466f81bea74df79c674433039ee962",
              "IPY_MODEL_b8ae688e7d0c478a86ffa0d955504da1",
              "IPY_MODEL_37b8dd12f46040edada15e5da27e9521"
            ],
            "layout": "IPY_MODEL_cd1d93c5e03e4562b4c048fc13242a63"
          }
        },
        "8d466f81bea74df79c674433039ee962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab86de505a845119c67b6b85e9fb20f",
            "placeholder": "​",
            "style": "IPY_MODEL_ef0f7b58f37a47a5a2b56b6f8e250fbc",
            "value": "law_qa_dataset.jsonl: 100%"
          }
        },
        "b8ae688e7d0c478a86ffa0d955504da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd8419c411a435da71122f464464ab6",
            "max": 32296382,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d48a733aad9430ab730ef1ddfefcafd",
            "value": 32296382
          }
        },
        "37b8dd12f46040edada15e5da27e9521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_892d2f9b6ea44980a5dd2fc5c283d0c9",
            "placeholder": "​",
            "style": "IPY_MODEL_1bdf636d0f8041df8b918ea5dd667ba3",
            "value": " 32.3M/32.3M [00:00&lt;00:00, 116MB/s]"
          }
        },
        "cd1d93c5e03e4562b4c048fc13242a63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab86de505a845119c67b6b85e9fb20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0f7b58f37a47a5a2b56b6f8e250fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fd8419c411a435da71122f464464ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d48a733aad9430ab730ef1ddfefcafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "892d2f9b6ea44980a5dd2fc5c283d0c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bdf636d0f8041df8b918ea5dd667ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20d59878c1da46a78f9505d59562f2f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bcb5e7e2f1444a9bd033181830132fb",
              "IPY_MODEL_f02842207877440594c1c65b5168e1a8",
              "IPY_MODEL_9505f13fa4884e4db0dbbd39dfaf77bf"
            ],
            "layout": "IPY_MODEL_1a17282e28b046d69087b513fe036dc0"
          }
        },
        "5bcb5e7e2f1444a9bd033181830132fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eea03459e784d91b773e965871a6f11",
            "placeholder": "​",
            "style": "IPY_MODEL_0889d33fc7f847b594a65d54238bc8af",
            "value": "Generating train split: 100%"
          }
        },
        "f02842207877440594c1c65b5168e1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89feafd427054373863d924c4afb0b2f",
            "max": 14819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b56edd3fc1441e98ba898d6fec53fe1",
            "value": 14819
          }
        },
        "9505f13fa4884e4db0dbbd39dfaf77bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb09551781d4fc3905824aa1a5cea7d",
            "placeholder": "​",
            "style": "IPY_MODEL_825a20c07bdd429b9e45dcee72e7e07f",
            "value": " 14819/14819 [00:00&lt;00:00, 54426.77 examples/s]"
          }
        },
        "1a17282e28b046d69087b513fe036dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eea03459e784d91b773e965871a6f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0889d33fc7f847b594a65d54238bc8af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89feafd427054373863d924c4afb0b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b56edd3fc1441e98ba898d6fec53fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cb09551781d4fc3905824aa1a5cea7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825a20c07bdd429b9e45dcee72e7e07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceb9b3e0580b4eada5547b4020263d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c9c6b58b67c40ca884220eaab1a1c49",
              "IPY_MODEL_11f12cc4e8f14c16b1fc3372d1fa4fb0",
              "IPY_MODEL_c23888845f9c408f8d2982477158d1a4"
            ],
            "layout": "IPY_MODEL_1cd4ea7c96c7468384fdcc126c4efd4d"
          }
        },
        "2c9c6b58b67c40ca884220eaab1a1c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c17114cdb05244a8b4c6f93e8ca30780",
            "placeholder": "​",
            "style": "IPY_MODEL_bcaeb08be821402ba66c6861f42cc14c",
            "value": "Filter: 100%"
          }
        },
        "11f12cc4e8f14c16b1fc3372d1fa4fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf7da85eafe442684f3dc37d14102ac",
            "max": 14819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_337e23b5143e4f32b7a681a3cc70eb24",
            "value": 14819
          }
        },
        "c23888845f9c408f8d2982477158d1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5307331971654e5aad669cae22891c31",
            "placeholder": "​",
            "style": "IPY_MODEL_0a5b4dc6be5e42f7b4e140b71be637aa",
            "value": " 14819/14819 [00:23&lt;00:00, 989.45 examples/s]"
          }
        },
        "1cd4ea7c96c7468384fdcc126c4efd4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c17114cdb05244a8b4c6f93e8ca30780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcaeb08be821402ba66c6861f42cc14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdf7da85eafe442684f3dc37d14102ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337e23b5143e4f32b7a681a3cc70eb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5307331971654e5aad669cae22891c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a5b4dc6be5e42f7b4e140b71be637aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2289d324cffa480792834f8d654020c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd9ec9cbd41e4d249ca6eb5482c1fe57",
              "IPY_MODEL_de0b22e262624f4493893b1cd1da25c0",
              "IPY_MODEL_220f1eb8151d416a9c7aec8b343e50f8"
            ],
            "layout": "IPY_MODEL_6783886927c640c0949f21e1f8a297fd"
          }
        },
        "bd9ec9cbd41e4d249ca6eb5482c1fe57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841334345ed94936a7696b740c8a8989",
            "placeholder": "​",
            "style": "IPY_MODEL_748728f90b4f4d009b73e9da8c347837",
            "value": "Map: 100%"
          }
        },
        "de0b22e262624f4493893b1cd1da25c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6759a358e6e47da97b030ee2c0a3ade",
            "max": 10535,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2621ccf612394064835c2726279c1bfc",
            "value": 10535
          }
        },
        "220f1eb8151d416a9c7aec8b343e50f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ba45454a3f4f2eb73de5e14a8de12a",
            "placeholder": "​",
            "style": "IPY_MODEL_3dfc3cdb99f7432db4a7f0722953d7bc",
            "value": " 10535/10535 [00:01&lt;00:00, 8882.36 examples/s]"
          }
        },
        "6783886927c640c0949f21e1f8a297fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841334345ed94936a7696b740c8a8989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "748728f90b4f4d009b73e9da8c347837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6759a358e6e47da97b030ee2c0a3ade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2621ccf612394064835c2726279c1bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36ba45454a3f4f2eb73de5e14a8de12a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dfc3cdb99f7432db4a7f0722953d7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b7af97ba1ca4c67843e35d38c164f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a08370d9e1554837966bd7693e826be2",
              "IPY_MODEL_67f5a1803b79436a9caffe19ea2fc104",
              "IPY_MODEL_25d1caa30f314443b66e10f4b358f020"
            ],
            "layout": "IPY_MODEL_53c5add6602c4c68a8946468e1306d55"
          }
        },
        "a08370d9e1554837966bd7693e826be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ddc44dabdd4f46b8afba98552a83ad",
            "placeholder": "​",
            "style": "IPY_MODEL_43a959afe63f4d21acb0f9f3f09301a8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "67f5a1803b79436a9caffe19ea2fc104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d50be7325946e99b94e0c95d35a38b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1182f737d0694499a9875d0f72b67bc0",
            "value": 4
          }
        },
        "25d1caa30f314443b66e10f4b358f020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4297597d3ba4d90bc87d3d9d05aa9fe",
            "placeholder": "​",
            "style": "IPY_MODEL_82a2c5a1ce434de39c057fa98619b0e2",
            "value": " 4/4 [01:22&lt;00:00, 19.13s/it]"
          }
        },
        "53c5add6602c4c68a8946468e1306d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ddc44dabdd4f46b8afba98552a83ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a959afe63f4d21acb0f9f3f09301a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27d50be7325946e99b94e0c95d35a38b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1182f737d0694499a9875d0f72b67bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4297597d3ba4d90bc87d3d9d05aa9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a2c5a1ce434de39c057fa98619b0e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "293907ade9f4442fadbf74f808a47897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b76bcb6619e485f8f30893750863cbf",
              "IPY_MODEL_1a63df4436f740e0ba0349e6fcd637f3",
              "IPY_MODEL_158aaf949454451db4225a20ab5173ed"
            ],
            "layout": "IPY_MODEL_3c0bd1e366e14ed58a6d3331d75f5005"
          }
        },
        "5b76bcb6619e485f8f30893750863cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fba71745e5a428183045dd6b75a21a4",
            "placeholder": "​",
            "style": "IPY_MODEL_d9720e71b2fb4c3485795f45f450e3fb",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "1a63df4436f740e0ba0349e6fcd637f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236defe62a344367ad123a818689e703",
            "max": 10535,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0c03a87a03d44629b474a26b70bb489",
            "value": 10535
          }
        },
        "158aaf949454451db4225a20ab5173ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98ea165228e44602954829659c21f66a",
            "placeholder": "​",
            "style": "IPY_MODEL_99900f9b273f4942a99bd6a90ba54dc0",
            "value": " 10535/10535 [00:17&lt;00:00, 784.67 examples/s]"
          }
        },
        "3c0bd1e366e14ed58a6d3331d75f5005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fba71745e5a428183045dd6b75a21a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9720e71b2fb4c3485795f45f450e3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "236defe62a344367ad123a818689e703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c03a87a03d44629b474a26b70bb489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98ea165228e44602954829659c21f66a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99900f9b273f4942a99bd6a90ba54dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}