{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasom222g/learn-LLM/blob/main/02_1_text_basic_preprocessing(%E1%84%8B%E1%85%A7%E1%86%BC%E1%84%8B%E1%85%A5%E1%84%86%E1%85%B5%E1%86%BE%E1%84%92%E1%85%A1%E1%86%AB%E1%84%80%E1%85%AE%E1%86%A8%E1%84%8B%E1%85%A5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi20RKmSio8x"
      },
      "source": [
        "#텍스트 전처리(Text Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXjGCQhziz_d"
      },
      "source": [
        "- 텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
        "- 텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
        "\n",
        "1) 토큰화(Tokenizing)\n",
        "- 텍스트를 자연어 처리를 위해 분리하는 것\n",
        "- 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"과 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"으로 구분  \n",
        "=> 이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭함\n",
        "\n",
        "2) 노이즈 및 불용어 처리\n",
        " - 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
        " - 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
        " - 불필요한 토큰을 제거함으로써 연산의 효율성을 높임\n",
        "\n",
        "3) 원형복원(Stemming & Lemmatization)\n",
        "- 각 토큰의 원형을 복원함으로써 토큰을 표준화 하여 불필요한 데이터 중복을 방지  \n",
        "(= 단어의 수를 줄일 수 있어 연산 효율성을 높임)  \n",
        "\n",
        " - 어간추출(Stemming): 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
        " - 표제어 추출(Lemmatization): 품사정보를 유지하여 표제어 추출\n",
        "\n",
        "4) 품사 부착(Pos Tagging)\n",
        "- 각 토큰에 품사 정보를 추가\n",
        "- 분석시에 불필요한 품사를 제거하거나(예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용\n",
        "\n",
        "5) 개체명 인식(NER, Named Entity Recognition)\n",
        "- 각 토큰의 개체구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
        "- 텍스트가 무엇과 관련되어 있는지 구분하기 위해 사용\n",
        "- 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7WKR67tj-j9"
      },
      "source": [
        "#1. 영문 전처리 연습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSfmG8RxkD_Y"
      },
      "source": [
        "NLTK lib(https://www.nltk.org/)사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E32YHTz0kJIr"
      },
      "source": [
        "## 실습용 영문기사 수집\n",
        "\n",
        "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
        "https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/#4a0130481f25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-hq5fi6keZZ"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3cyyQxSkieU"
      },
      "source": [
        "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/#4a0130481f25'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCd6NTG8kpco",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "302aeb4e-f511-49de-d1dc-622a880ac741"
      },
      "source": [
        "eng_news = soup.select('p') #[class = 'speakable-paragraph']\n",
        "eng_text = eng_news[3].get_text()\n",
        "\n",
        "eng_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', Senior Contributor. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6j_xjrk1Zn"
      },
      "source": [
        "## 1) 영문토큰화\n",
        "https://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0poQI4Fk4bF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcaaed2c-c0aa-4765-9214-ad10c0bcc9e4"
      },
      "source": [
        "# 영문 자연어 처리 및 문서 분석용 파이썬 패키지\n",
        "\n",
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_tokenize(): 단어와 구두점(온점, 컴마, 물음표, 세미콜론, 느낌표 등과 같은 기호)으로 단어 토큰화\n",
        "#LineTokenizer(): 줄바꿈 문자('\\n')를 기준으로 토큰화\n",
        "#sent_tokenize(): 문장 기준으로 토큰화\n",
        "#SpaceTokenizer(): 공백 기준으로 토큰화\n",
        "\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # 말뭉치 다운로드\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q9Nv1aX6dOw",
        "outputId": "132d8a68-d511-47f8-b363-de0dda3a0571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"James is working at Disney in London\"\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6iZPi0S62zs",
        "outputId": "58590b36-5ca5-46f0-cc51-d1c7617e00bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'is', 'working', 'at', 'Disney', 'in', 'London']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r4iZyTBlqeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a15e3085-49a1-4660-df75-0f7bb827e2f0"
      },
      "source": [
        "#WordPunctTokenizer(): 알파벳과 알파벳이 아닌 문자를 구분하여 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "text =  \"James is working at Disney in London\"'\n",
        "wordpuncttoken = WordPunctTokenizer().tokenize(text)\n",
        "print(wordpuncttoken)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'is', 'working', 'at', 'Disney', 'in', 'London']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKBHE9TjlyES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d32024-b787-4c68-ee96-dbac5313fe8e"
      },
      "source": [
        "#TreebankWordTokenizer(): 정규표현식에 기반한 토큰화\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "text =  \"James is working at Disney in London\"\n",
        "treebankwordtoken = TreebankWordTokenizer().tokenize(text)\n",
        "print(treebankwordtoken)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'is', 'working', 'at', 'Disney', 'in', 'London']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjM5QL_joCBW"
      },
      "source": [
        "## 2) 노이즈 및 불용어(Stopword) 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 노이즈 처리 기준\n",
        "\n",
        "- 특수 문자 및 기호  \n",
        "    HTML/XML 태그 (<div>, <p> 등)  \n",
        "    구두점 (.,!?:;)  \n",
        "    괄호, 대시, 슬래시 등 (()[]{}-/)  \n",
        "    이모티콘과 이모지 (😊, :), :-P)  \n",
        "\n",
        "- 숫자와 관련 표현  \n",
        "    일반 숫자 (1, 2, 3...)  \n",
        "    날짜 형식 (2023-03-07, 07/03/2023)  \n",
        "    금액 표시 ($100, ₩10,000)  \n",
        "\n",
        "- 잘못된 입력 및 오류  \n",
        "    오타 및 맞춤법 오류  \n",
        "    인코딩 오류 (깨진 문자)  \n",
        "\n",
        "- 포맷 관련 요소  \n",
        "    URL (http://example.com)  \n",
        "    이메일 주소 (user@example.com)  \n",
        "    파일 경로 (/usr/local/bin)  \n",
        "    사용자 ID, 해시태그 (@user, #hashtag)  \n",
        "\n",
        "- 저빈도 출현 요소  \n",
        "    코퍼스에서 1-5회 미만 등장하는 단어  \n",
        "    특정 문서에만 등장하는 희귀 단어\n",
        "  \n",
        "⭐️ 코퍼스: 말뭉치로 전체 토큰 목록을 의미"
      ],
      "metadata": {
        "id": "6X6Z6FzW_MOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 불용어에 해당하는 것들\n",
        "\n",
        "- 일반적 고빈도 동사  \n",
        "    be 동사 (am, is, are, was, were)  \n",
        "    have/has/had  \n",
        "    do/does/did  \n",
        "\n",
        "- 분석 목적에 따른 중립(일반적인) 단어  \n",
        "    감성분석에서 중립적인 표현 단어  \n",
        "    주제 분류에 영향이 적은 일반 단어  \n",
        "\n",
        "- 메타 단어  \n",
        "    교과서 형식의 텍스트에서 \"chapter\", \"figure\"  \n",
        "    뉴스에서 \"reported\", \"according\"  \n",
        "    학술 논문에서 \"study\", \"research\", \"analysis\"  \n",
        "\n",
        "- 고빈도 단어  \n",
        "    코퍼스의 상위 1-5%를 차지하는 단어  \n",
        "    대부분의 문서(80% 이상)에 등장하는 단어  \n",
        "    TF-IDF 값이 매우 낮은 단어  \n",
        "\n",
        "- 특정 언어별 불용어  \n",
        "    한국어: 이, 그, 저, 것, 등, 및, 에서, 으로  \n",
        "    영어: this, that, these, those, such, what  \n",
        "    일본어: これ, それ, あれ, この, その  \n",
        "\n",
        "- 문법적 기능어  \n",
        "    관사 (a, an, the)  \n",
        "    전치사 (in, on, at, by, with)  \n",
        "    접속사 (and, but, or, so)  \n",
        "    조동사 (can, will, should)  \n",
        "    대명사 (I, you, he, she, they)"
      ],
      "metadata": {
        "id": "AqwbHwbiAJXD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SXnMij3oaZN"
      },
      "source": [
        "stop_words = [\"at\", \"be\", \"able\", \"in\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55cajMcYoeIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea4ecdb-fc4d-4c76-f1b8-83f6dc2336a8"
      },
      "source": [
        "word_tokens = []\n",
        "for tag in treebankwordtoken:\n",
        "    if len(tag) > 1:\n",
        "        if tag not in stop_words:\n",
        "            word_tokens.append(tag)\n",
        "\n",
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['James', 'is', 'working', 'Disney', 'London']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최빈어 조회: 최빈어를 조회하여 불용어 제거\n",
        "from collections import Counter\n",
        "Counter(word_tokens).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqEiFiEJA2sb",
        "outputId": "06ea58cc-826a-456a-8f8b-482189ec0187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('James', 1),\n",
              " ('is', 1),\n",
              " ('working', 1),\n",
              " ('at', 1),\n",
              " ('Disney', 1),\n",
              " ('in', 1),\n",
              " ('London', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문제\n",
        "word_tokens에서 빈도수가 5개 이상인 토큰은 삭제"
      ],
      "metadata": {
        "id": "hK0w_9EddzGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The cat sat on the mat while the dog slept by the door. The cat noticed the bird on the tree and watched it carefully. The dog woke up when the mailman arrived at the door. The children played with the cat and the dog in the garden all afternoon.'\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "\n",
        "result = []\n",
        "\n",
        "# 빈도수 조회\n",
        "token_numbers = Counter(word_tokens).most_common()\n",
        "\n",
        "for word, num in token_numbers:\n",
        "  # print(word)\n",
        "  # print(num)\n",
        "  # print('='*30)\n",
        "  if num < 5:\n",
        "    result.append(word)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeA0ctmUdysm",
        "outputId": "273fe813-bbcc-40b2-be6d-073c8dca40c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " '.',\n",
              " 'cat',\n",
              " 'dog',\n",
              " 'on',\n",
              " 'door',\n",
              " 'and',\n",
              " 'sat',\n",
              " 'mat',\n",
              " 'while',\n",
              " 'slept',\n",
              " 'by',\n",
              " 'noticed',\n",
              " 'bird',\n",
              " 'tree',\n",
              " 'watched',\n",
              " 'it',\n",
              " 'carefully',\n",
              " 'woke',\n",
              " 'up',\n",
              " 'when',\n",
              " 'mailman',\n",
              " 'arrived',\n",
              " 'at',\n",
              " 'children',\n",
              " 'played',\n",
              " 'with',\n",
              " 'in',\n",
              " 'garden',\n",
              " 'all',\n",
              " 'afternoon']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvrF1piqm93I"
      },
      "source": [
        "## 3) 원형복원\n",
        "각 토큰의 원형을 복원하여 표준화한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIn9P5aWnExP"
      },
      "source": [
        "### 3-1) 어간추출(Stemming)\n",
        "- 규칙에 기반하여 토큰을 표준화\n",
        "- ing제거, ful제거 등  \n",
        "http://www.nltk.org/api/nltk.chunk.html\n",
        "\n",
        "규칙 상세: http://www.nltk.org/api/nltk.chunk.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzmVDuNjnQC2"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4umNUPgnWCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7c53cb-c024-4f89-92c9-bc59c4afea60"
      },
      "source": [
        "print(\"running -> \" + ps.stem(\"running\"))\n",
        "print(\"believes -> \"+ps.stem('believes'))\n",
        "print('using ->' + ps.stem(\"using\"))\n",
        "print(\"conversation ->\" + ps.stem('conversation'))\n",
        "print('organization ->'+ ps.stem('organization'))\n",
        "print('studies -> '+ ps.stem(\"studies\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "believes -> believ\n",
            "using ->use\n",
            "conversation ->convers\n",
            "organization ->organ\n",
            "studies -> studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4q19tr7nssB"
      },
      "source": [
        "### 3-2) 표제어 추출(Lemmatization)\n",
        "- 품사정보를 유지하여 토큰을 표준화\n",
        "- 어간 추출보다 완성형 토큰 보관가능\n",
        "\n",
        "http://www.nltk.org/api/nltk.chunk.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmHHrOGYnz75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9115ca9-96bc-4d57-f602-d56b1402a42d"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W__Dwo5vn3LT"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP54tNuCn8Xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf52eab8-13c2-4d9b-de4c-3e83bd63c2ee"
      },
      "source": [
        "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
        "print(\"believes -> \"+wl.lemmatize('believes'))\n",
        "print('using ->' + wl.lemmatize(\"using\"))\n",
        "print(\"conversation ->\" + wl.lemmatize('conversation'))\n",
        "print('organization ->'+ wl.lemmatize('organization'))\n",
        "print('studies -> '+ wl.lemmatize(\"studies\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> running\n",
            "believes -> belief\n",
            "using ->using\n",
            "conversation ->conversation\n",
            "organization ->organization\n",
            "studies -> study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 문제\n",
        "어간추출을 이용하여 원형복원해보세요."
      ],
      "metadata": {
        "id": "_COhyCvrBDb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens"
      ],
      "metadata": {
        "id": "i89xSUqIBBXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96068e01-bc0c-4f75-fdb9-fd1ec2c03466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['James', 'is', 'working', 'Disney', 'London']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer_word_tokens = [wl.lemmatize(word) for word in word_tokens]\n",
        "lemmatizer_word_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vkJvM7k0gro",
        "outputId": "6b7fbc61-3469-41e7-8131-8f7d50517921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['James', 'is', 'working', 'Disney', 'London']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo-tGurmmPOI"
      },
      "source": [
        "## 4) 영문 품사 부착(PoS tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOxdStzHmTPD"
      },
      "source": [
        "분리한 토큰마다 품사를 부착한다.   \n",
        "https://www.nltk.org/api/nltk.tag.html\n",
        "\n",
        "태그목록: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
        "\n",
        "## 자주 사용되는 품사종류\n",
        "- NN: 명사, 단수형 또는 불가산\n",
        "- NNS: 명사, 복수형\n",
        "- NNP: 고유 명사, 단수형\n",
        "- VB: 동사, 원형\n",
        "- VBD: 동사, 과거형\n",
        "- VBG: 동사, 현재분사/동명사\n",
        "- VBN: 동사, 과거분사\n",
        "- VBZ: 동사, 3인칭 단수 현재\n",
        "- JJ: 형용사\n",
        "- RB: 부사\n",
        "- IN: 전치사 또는 종속접속사\n",
        "- DT: 한정사(관사 등)\n",
        "- CC: 등위접속사\n",
        "- PRP: 인칭 대명사\n",
        "- TO: to (전치사/부정사 표지)\n",
        "- MD: 조동사"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng') # 품사 맵핑의 기준이 되는 데이터"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if02kpvX76yp",
        "outputId": "c9df6f8b-2f6f-4d46-cdcb-d54314bc208f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY4p27-6mdXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063e2f62-e1d3-4e5b-8af1-6e08b620bf50"
      },
      "source": [
        "tagged_tokens = pos_tag(word_tokens)\n",
        "print(tagged_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('Disney', 'NNP'), ('London', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u65AMKkDoKZT"
      },
      "source": [
        "# 불필요한 품사 제거\n",
        "stop_pos = [\"IN\", \"CC\", \"UH\", \"TO\", \"MD\", \"DT\", \"VBZ\", \"VBP\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 문제\n",
        "stop_pos에 해당하는 제거해보세요"
      ],
      "metadata": {
        "id": "pqAGyvXx8We2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_pos에 포함되지 않는 품사만 추출\n",
        "result =  [(token, part) for token, part in tagged_tokens if part not in stop_pos]\n",
        "result"
      ],
      "metadata": {
        "id": "WT9Fje0g8VRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e080b753-c05a-4e0e-bdb2-3d2594aa278b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('James', 'NNP'), ('working', 'VBG'), ('Disney', 'NNP'), ('London', 'NNP')]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIyviWpQmszG"
      },
      "source": [
        "##5) 개체명 인식(NER, Named Entity Recognition)\n",
        "http://www.nltk.org/api/nltk.chunk.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "개체명 인식(Named Entity Recognition)이란 말 그대로 이름을 가진 개체(named entity)를 인식하겠다는 것을 의미합니다.   \n",
        "즉, 어떤 이름을 의미하는 단어를 보고는 그 단어가 어떤 유형인지를 인식하는 것을 말합니다."
      ],
      "metadata": {
        "id": "KwU2RBaGYWPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예를 들어 \b제임스는 2018년에 골드만삭스에 입사했다. 라는 문장이 있을 때, 사람(person), 조직(organization), 시간(time)에 대해 개체명 인식을 수행하는 모델이라면 다음과 같은 결과를 보여줍니다.\n",
        "\n",
        "- 제임스 : 사람\n",
        "- 2018년 : 시간\n",
        "- 골드만삭스 : 조직"
      ],
      "metadata": {
        "id": "oUxiL3o_YdYw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLlSBjPmzey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52f789e-172d-4eee-9c52-d7434b9be34b"
      },
      "source": [
        "nltk.download('words') # 영어 단어 목록을 다운로드 (참조 데이터로 활용)\n",
        "nltk.download('maxent_ne_chunker_tab') # 개체명 인식(NER)을 위한 최대 엔트로피 기반 모델 다운로드 (개체명 식별)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mayQjpNJ58-W",
        "outputId": "f37677a4-3783-4910-d034-a2e9bdd0bf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('James', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('working', 'VBG'),\n",
              " ('Disney', 'NNP'),\n",
              " ('London', 'NNP')]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDAZMQzCm4AZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16dcb573-eb11-462c-f930-88368ba2734a"
      },
      "source": [
        "# 개체명 인식해주는 라이브러리\n",
        "from nltk import ne_chunk\n",
        "ne_tokens = ne_chunk(tagged_tokens)\n",
        "print(ne_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON James/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  (PERSON Disney/NNP London/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "James는 PERSON(사람), Disney는 조직(ORGANIZATION), London은 위치(GPE)"
      ],
      "metadata": {
        "id": "wesTWfS5ZBUW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkcMRwNYpBCu"
      },
      "source": [
        "#2. 한글 전처리 실습\n",
        "\n",
        "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화해야 한다.\n",
        "\n",
        "- 한글 토큰화 단위: 형태소 - 의미를 가진 최소단위"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq6znXciqK4u"
      },
      "source": [
        "## 실습용 한글기사 수집\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyLgGOc1qOXg"
      },
      "source": [
        "- https://n.news.naver.com/mnews/article/016/0002032876?sid=105"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL7tkeFmwtid"
      },
      "source": [
        "kor_text = '''포스코ICT는 25일 경기도 판교 사옥에서 청소년들이 개발한 인공지능(AI) 기반 애플리케이션을 직접 선보이는 ‘2022 AI 유스 챌린지(Youth Challenge)’를 개최했다. 청소년들은 기성세대들이 생각하지 못했던 참신한 아이디어를 바탕으로 AI 시스템을 개발해 관심을 모았다.\n",
        "\n",
        "예선을 거쳐 본선에 오른 전국 중·고교 7개팀은 지난 달부터 포스코ICT 소속 AI 엔지니어의 코칭 속에 개발한 AI 시스템을 직접 시연했다. 심사는 카이스트, 포스텍, 연세대 교수 등으로 구성된 평가위원들과 포스코ICT 직원들로 구성된 내부 평가위원들이 맡았다. 우수작은 과기정통부장관상(1팀), 포스코ICT사장상(1팀), 지능정보산업협회장상(1팀), 우수상(2팀), 장려상(2팀) 등으로 나눠 각각 시상했다. 포스코ICT는 AI 기술을 접목해 환경·안전·사회문제를 해결할 수 있는 청소년들의 아이디어를 발굴하기 위해 이번 공모전을 개최했다. 평소 AI 기술을 접할 기회가 부족했던 청소년들은 멘토링을 통해 산업현장에서 실제 활용되는 기술을 자연스럽게 체험하고 진로 결정에도 도움을 받을 수 있었다.\n",
        "\n",
        "이번 공모전에 참여한 김희주 학생은 “막연히 생각했던 아이디어를 실제 AI 전문가들과 구체화해나가며 접하지 못했던 지식과 기술을 배울 수 있었고 AI 분야에 더 큰 관심을 가지게 됐다”고 말했다.\n",
        "\n",
        "학생들의 멘토를 맡은 포스코ICT 최영철 연구원은 “학생들의 열정과 창의적인 아이디어를 접하며 새로운 자극을 받아 재미있게 멘토링에 참여했다”며 “그동안 AI 분야에서 쌓아온 재능을 기부해 AI 전문가를 꿈꾸는 청소년들에게 전해줄 수 있어 의미 있었다”고 밝혔다. 한편, 포스코ICT는 아주대학교 대학원과 ‘AI 전문인력 양성을 위한 업무협약’을 체결해 프로그램을 운영하고 있다. 지난 24일에는 인공지능학과 대학원 과정에 재학 중인 우수 인재를 선발해 장학금을 지급했다.\n",
        "\n",
        "이번에 선발된 학생에게는 졸업 때까지 매달 장학금을 지급하고, 포스코ICT의 AI 전문가와의 1대1 멘토링 및 각종 기술 교육, 세미나 참석을 비롯해 채용 기회까지 제공할 계획이다. 김현일 기자'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjdVP8sYw3Lt"
      },
      "source": [
        "## 1) 한글 토큰화 및 형태소 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1n6knExDK8"
      },
      "source": [
        "한글 자연어 처리기 비교  \n",
        "https://konlpy.org/ko/latest/morph/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 형태소 분석기별 메서드 정리\n",
        "출처 : https://konlpy-ko.readthedocs.io/ko/v0.4.3/\n",
        "\n",
        "---\n",
        "1) Hannanum : KAIST에서 개발   \n",
        "- analyze : 다양한 형태학적 후보를 반환\n",
        "- morphs : 형태소 토큰화\n",
        "- nouns : 명사 토큰만 반환\n",
        "- pos : 형태소에 품사 태깅하여 반환\n",
        "---  \n",
        "2) Kkma : 서울대에서 개발\n",
        "- morphs : 형태소 토큰화\n",
        "- nouns : 명사 토큰만 반환\n",
        "- pos : 형태소에 품사 태깅하여 반환\n",
        "- sentences : 문장 토큰화\n",
        "---  \n",
        "3) Komoran : Shineware팀에서 개발\n",
        "- morphs : 형태소 토큰화\n",
        "- nouns : 명사 토큰만 반환\n",
        "- pos : 형태소에 품사 태깅하여 반환\n",
        "---\n",
        "4) Mecab : 교토대에서 일본어용으로 개발된 분석기, 은전 프로젝트에 의해 한국어용 개발\n",
        "- morphs : 형태소 토큰화\n",
        "- nouns : 명사 토큰만 반환\n",
        "- pos : 형태소에 품사 태깅하여 반환\n",
        "---\n",
        "5) Okt(Twitter) : Will Hohyon Ryu가 개발\n",
        "- morphs : 형태소 토큰화\n",
        "- nouns : 명사 토큰만 반환\n",
        "- phrases : 다양한 형태학적 후보를 반환\n",
        "- pos : 형태소에 품사 태깅하여 반환\n"
      ],
      "metadata": {
        "id": "4X_NVQNnZShh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6V9Lc70xLxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b16c0a6-85e0-49f3-8ab1-f6335f561a34"
      },
      "source": [
        "#konlpy 설치\n",
        "# 한국어 자연어 전처리용 라이브러리로 형태소 분석 가능\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgooNkeSxbTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e4a08c-b3be-4014-f0f7-cfb72a448f28"
      },
      "source": [
        "#코모란(Komoran) 토큰화\n",
        "\n",
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 꺠닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있다.\"\n",
        "komoran_tokens = komoran.morphs(kor_text)\n",
        "print(komoran_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '꺠닫지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-hCuM8XxvNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483983fc-5626-47c6-858e-e71040f2ad08"
      },
      "source": [
        "#한나눔(hannanum) 토큰화\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 꺠닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있다.\"\n",
        "hannanum_tokens = hannanum.morphs(kor_text)\n",
        "print(hannanum_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '꺠닫지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqb3TjN-yBnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c679e5-5355-4072-f0ce-e34683ade3b7"
      },
      "source": [
        "#Okt 토큰화\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "okt_tokens = okt.morphs(kor_text)\n",
        "print(okt_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '꺠닫', '지', '못', '하고', '인간', '과', '대화', '를', '계속', '할', '수', '있다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '될', '수', '있다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIeDF4EAyW2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88aaccc2-210d-4fe4-9d80-ed4d85ac64d9"
      },
      "source": [
        "# Kkma 토큰화\n",
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma_tokens = kkma.morphs(kor_text)\n",
        "print(kkma_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '꺠', '닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kor_text = '그래욬ㅋㅋㅋ잘해봅시다! 잘하고 있지만 더 잘해보면 좋지 않을까라고 생각하는 중'"
      ],
      "metadata": {
        "id": "4Ah5zmnZKacQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "m7guPNv7O4oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.morphs(kor_text, norm = False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUVZyCMKONn-",
        "outputId": "1462bdb9-89c1-41bd-d2ef-e088253715b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['그래욬', 'ㅋㅋㅋ', '잘', '해봅시다', '!', '잘', '하고', '있지만', '더', '잘', '해보면', '좋지', '않을까', '라고', '생각', '하는', '중']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.morphs(kor_text, norm=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y157zVjqJwvf",
        "outputId": "8268c59f-9f6c-4537-9151-54f21c8af668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['그래요', 'ㅋㅋㅋ', '잘', '해봅시다', '!', '잘', '하고', '있지만', '더', '잘', '해보면', '좋지', '않을까', '라고', '생각', '하는', '중']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.morphs(kor_text, stem = True))\n",
        "# 좋지 -> 좋다\n",
        "# 않을까 -> 않다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abzno3Q_KBYB",
        "outputId": "21ddf9b7-7db2-4309-cfc7-d384afd5d02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['그래욬', 'ㅋㅋㅋ', '잘', '해보다', '!', '잘', '하다', '있다', '더', '잘', '해보다', '좋다', '않다', '라고', '생각', '하다', '중']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jigjdTzvyrCo"
      },
      "source": [
        "## 2) 한글 품사 부착(PoS Tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLvx-C3Jyurq"
      },
      "source": [
        "Pos Tag 목록\n",
        "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uz-1t8FyzrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0511d2ca-1f15-4115-f9a1-953d8c524c5b"
      },
      "source": [
        "# 코모란(Komoran) 품사 태깅\n",
        "komoran_tags = []\n",
        "for token in komoran_tokens:\n",
        "  komoran_tags += komoran.pos(token)\n",
        "print(komoran_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'MM'), ('컴퓨터', 'NNG'), ('오', 'VV'), ('아', 'EC'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'MM'), ('있', 'VV'), ('달', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('을', 'NNG'), ('꺠닫지', 'NA'), ('못', 'MAG'), ('하', 'MAG'), ('고', 'MM'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'JKO'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('지능', 'NNP'), ('적', 'NNB'), ('이', 'MM'), ('ㄴ', 'JX'), ('것', 'NNB'), ('으로', 'JKB'), ('간주', 'NNG'), ('되', 'NNB'), ('ㄹ', 'NA'), ('수', 'NNB'), ('있', 'VV'), ('다', 'MAG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5IRmwubzDSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b870db4-47f0-4e57-ddc9-fb03ee144e4d"
      },
      "source": [
        "# 한나눔(Hannanum) 품사 태깅\n",
        "hannanum_tags = []\n",
        "for token in hannanum_tokens:\n",
        "  hannanum_tags += hannanum.pos(token)\n",
        "\n",
        "print(hannanum_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'N'), ('이', 'M'), ('컴퓨터', 'N'), ('와', 'I'), ('대화', 'N'), ('하', 'P'), ('고', 'E'), ('있', 'N'), ('다', 'M'), ('는', 'J'), ('것', 'N'), ('을', 'N'), ('꺠닫지', 'N'), ('못하', 'P'), ('어', 'E'), ('고', 'M'), ('인간', 'N'), ('과', 'N'), ('대화', 'N'), ('를', 'N'), ('계속', 'M'), ('하', 'I'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다면', 'N'), ('컴퓨터', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('지능적', 'N'), ('이', 'M'), ('ㄴ', 'N'), ('것', 'N'), ('으', 'N'), ('로', 'J'), ('간주', 'N'), ('되', 'N'), ('ㄹ', 'N'), ('수', 'N'), ('있', 'N'), ('다', 'M'), ('.', 'S')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eKGCe0CzdDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd019731-2d7f-4942-956a-9a217e2b6894"
      },
      "source": [
        "# Okt 품사 태깅\n",
        "okt_tags = []\n",
        "for token in okt_tokens:\n",
        "  okt_tags += okt.pos(token)\n",
        "print(okt_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'Noun'), ('이', 'Noun'), ('컴퓨터', 'Noun'), ('와', 'Verb'), ('대화', 'Noun'), ('하고', 'Verb'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('꺠닫', 'Noun'), ('지', 'Verb'), ('못', 'Noun'), ('하고', 'Verb'), ('인간', 'Noun'), ('과', 'Noun'), ('대화', 'Noun'), ('를', 'Noun'), ('계속', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있다면', 'Adjective'), ('컴퓨터', 'Noun'), ('는', 'Verb'), ('지능', 'Noun'), ('적', 'Noun'), ('인', 'Noun'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('될', 'Verb'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGjlKUtFzuxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c9e7c7-4d02-4817-f2d8-c955de85f3a3"
      },
      "source": [
        "#Kkma 품사 태깅\n",
        "kkma_tags = []\n",
        "for token in kkma_tokens:\n",
        "  kkma_tags += kkma.pos(token)\n",
        "print(kkma_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'NNG'), ('컴퓨터', 'NNG'), ('오', 'VA'), ('아', 'ECS'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'NNG'), ('있', 'VA'), ('달', 'VV'), ('는', 'ETD'), ('것', 'NNB'), ('을', 'NNG'), ('꺠', 'UN'), ('닫', 'VV'), ('지', 'NNG'), ('못하', 'VX'), ('고', 'NNG'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'UN'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('지능', 'NNG'), ('적', 'NNG'), ('이', 'NNG'), ('ㄴ', 'NNG'), ('것', 'NNB'), ('으', 'UN'), ('로', 'JKM'), ('간주', 'NNG'), ('되', 'VA'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다', 'NNG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXRAxzi2z291"
      },
      "source": [
        "## 3) 노이즈 및 불용어(StopWord) 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj8IcYhbz8ht"
      },
      "source": [
        "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1k1_HYk0A2F"
      },
      "source": [
        "# 불용어 처리\n",
        "stop_pos = ['Suffix','Punctuation','Josa','Foreign','Alpha','Number'] # 품사"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQiZkyyH0IEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c608c271-6ee2-49af-d645-be92076e897f"
      },
      "source": [
        "#최빈어 조회: 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(okt_tags).most_common()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('인간', 'Noun'), 2),\n",
              " (('컴퓨터', 'Noun'), 2),\n",
              " (('대화', 'Noun'), 2),\n",
              " (('하고', 'Verb'), 2),\n",
              " (('것', 'Noun'), 2),\n",
              " (('수', 'Noun'), 2),\n",
              " (('이', 'Noun'), 1),\n",
              " (('와', 'Verb'), 1),\n",
              " (('있다는', 'Adjective'), 1),\n",
              " (('을', 'Josa'), 1),\n",
              " (('꺠닫', 'Noun'), 1),\n",
              " (('지', 'Verb'), 1),\n",
              " (('못', 'Noun'), 1),\n",
              " (('과', 'Noun'), 1),\n",
              " (('를', 'Noun'), 1),\n",
              " (('계속', 'Noun'), 1),\n",
              " (('할', 'Verb'), 1),\n",
              " (('있다면', 'Adjective'), 1),\n",
              " (('는', 'Verb'), 1),\n",
              " (('지능', 'Noun'), 1),\n",
              " (('적', 'Noun'), 1),\n",
              " (('인', 'Noun'), 1),\n",
              " (('으로', 'Josa'), 1),\n",
              " (('간주', 'Noun'), 1),\n",
              " (('될', 'Verb'), 1),\n",
              " (('있다', 'Adjective'), 1),\n",
              " (('.', 'Punctuation'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU7AEdyY0QfW"
      },
      "source": [
        "stop_words = ['의','와','이','로','두고','들','를','은','과','수','했다','것','있는','한다','하는','그','있다']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(okt_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbC8yaNfHfd2",
        "outputId": "9ac7887a-8725-4777-b9d1-5b0447a1b785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeSfa8A-0VEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66d4489-1a91-4ae6-feb4-3b6cb3ef0453"
      },
      "source": [
        "word =[]\n",
        "for tag in okt_tags:\n",
        "  if tag[1] not in stop_pos:\n",
        "    if tag[0] not in stop_words:\n",
        "      word.append(tag[0])\n",
        "\n",
        "print(word)\n",
        "len(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '컴퓨터', '대화', '하고', '있다는', '꺠닫', '지', '못', '하고', '인간', '대화', '계속', '할', '있다면', '컴퓨터', '는', '지능', '적', '인', '간주', '될']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "\n",
        "for tag, pos in okt_tags:\n",
        "  if pos not in stop_pos and tag not in stop_words: # 품사 체크\n",
        "    result.append(tag)\n",
        "print(result)\n",
        "len(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkXFXklyHy_m",
        "outputId": "a05ab13a-26b2-443d-b0eb-6d9bd76087a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '컴퓨터', '대화', '하고', '있다는', '꺠닫', '지', '못', '하고', '인간', '대화', '계속', '할', '있다면', '컴퓨터', '는', '지능', '적', '인', '간주', '될']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문제\n",
        "kkma를 이용하여 빈도수가 3이상이거나 아래 품사에 해당하는 토큰들 제거\n",
        "\n",
        "- ECS, VA, ETD"
      ],
      "metadata": {
        "id": "tTjSLun9GkDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kor_text = \"인간이 컴퓨터와 대화하고 있다는 것을 꺠닫지 못하고 인간과 대화를 계속할 수 있다면 컴퓨터는 지능적인 것으로 간주될 수 있다.\"\n",
        "\n",
        "# Kkma 토큰화\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "kkma = Kkma()\n",
        "kkma_tokens = kkma.morphs(kor_text)\n",
        "print(kkma_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mriAnLjTIgau",
        "outputId": "b52aa0a1-4079-4378-d210-7cdd463eeba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '꺠', '닫', '지', '못하', '고', '인간', '과', '대화', '를', '계속', '하', 'ㄹ', '수', '있', '다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '되', 'ㄹ', '수', '있', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_tags = []\n",
        "for token in kkma_tokens:\n",
        "  tokens_tags.append(kkma.pos(token)[0])\n",
        "\n",
        "print(tokens_tags)\n",
        "\n",
        "stop_pos = ['ECS', 'VA', 'ETD']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpKUK3yFJHki",
        "outputId": "3af47cf8-f67b-43a4-862c-38ab24754bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'NNG'), ('컴퓨터', 'NNG'), ('오', 'VA'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'NNG'), ('있', 'VA'), ('달', 'VV'), ('것', 'NNB'), ('을', 'NNG'), ('꺠', 'UN'), ('닫', 'VV'), ('지', 'NNG'), ('못하', 'VX'), ('고', 'NNG'), ('인간', 'NNG'), ('과', 'NNG'), ('대화', 'NNG'), ('를', 'UN'), ('계속', 'MAG'), ('하', 'NNG'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다면', 'NNG'), ('컴퓨터', 'NNG'), ('늘', 'VA'), ('지능', 'NNG'), ('적', 'NNG'), ('이', 'NNG'), ('ㄴ', 'NNG'), ('것', 'NNB'), ('으', 'UN'), ('간주', 'NNG'), ('되', 'VA'), ('ㄹ', 'NNG'), ('수', 'NNG'), ('있', 'VA'), ('다', 'NNG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_pos_cnt = Counter(tokens_tags).most_common()\n",
        "token_pos_cnt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yfmc-r8JsaD",
        "outputId": "f09d6e7b-cf63-4ffa-c00e-04cce02c6ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('있', 'VA'), 3),\n",
              " (('인간', 'NNG'), 2),\n",
              " (('이', 'NNG'), 2),\n",
              " (('컴퓨터', 'NNG'), 2),\n",
              " (('대화', 'NNG'), 2),\n",
              " (('하', 'NNG'), 2),\n",
              " (('고', 'NNG'), 2),\n",
              " (('것', 'NNB'), 2),\n",
              " (('ㄹ', 'NNG'), 2),\n",
              " (('수', 'NNG'), 2),\n",
              " (('오', 'VA'), 1),\n",
              " (('달', 'VV'), 1),\n",
              " (('을', 'NNG'), 1),\n",
              " (('꺠', 'UN'), 1),\n",
              " (('닫', 'VV'), 1),\n",
              " (('지', 'NNG'), 1),\n",
              " (('못하', 'VX'), 1),\n",
              " (('과', 'NNG'), 1),\n",
              " (('를', 'UN'), 1),\n",
              " (('계속', 'MAG'), 1),\n",
              " (('다면', 'NNG'), 1),\n",
              " (('늘', 'VA'), 1),\n",
              " (('지능', 'NNG'), 1),\n",
              " (('적', 'NNG'), 1),\n",
              " (('ㄴ', 'NNG'), 1),\n",
              " (('으', 'UN'), 1),\n",
              " (('간주', 'NNG'), 1),\n",
              " (('되', 'VA'), 1),\n",
              " (('다', 'NNG'), 1),\n",
              " (('.', 'SF'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}